{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks to https://www.kaggle.com/siavrez/wavenet-keras and Sergey Bryansky.\n",
    "# You can take a look at Sergey's kernel [here](https://www.kaggle.com/sggpls/shifted-rfc-pipeline) or [here](https://www.kaggle.com/sggpls/wavenet-with-shifted-rfc-proba). Also, Sergey's [data is here.](https://www.kaggle.com/sggpls/ion-shifted-rfc-proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T07:31:16.184704Z",
     "start_time": "2020-04-24T07:31:16.177734Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'plaidml.keras.backend'\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.callbacks import Callback, LearningRateScheduler, EarlyStopping\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import losses, models, optimizers\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T07:28:46.709676Z",
     "start_time": "2020-04-24T07:28:46.706091Z"
    }
   },
   "outputs": [],
   "source": [
    "# configurations and main hyperparammeters\n",
    "EPOCHS = 110\n",
    "NNBATCHSIZE = 16\n",
    "GROUP_BATCH_SIZE = 4000\n",
    "SEED = 321\n",
    "LR = 0.001\n",
    "SPLITS = 5\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T18:56:36.783592Z",
     "start_time": "2020-04-24T07:33:42.257090Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data Started...\n",
      "Reading and Normalizing Data Completed\n",
      "Creating Features\n",
      "Feature Engineering Started...\n",
      "Feature Engineering Completed...\n",
      "Training Wavenet model with 5 folds of GroupKFold Started...\n",
      "Our training dataset shape is (1000, 4000, 19)\n",
      "Our validation dataset shape is (250, 4000, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"metal_amd_radeon_pro_560x.0\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2102 of 3912 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 3792 of 3912 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 109s - loss: 0.5880 - acc: 0.8319 - val_loss: 0.8162 - val_acc: 0.8614\n",
      "Epoch 2/110\n",
      " - 71s - loss: 0.1999 - acc: 0.9529 - val_loss: 0.4384 - val_acc: 0.9453\n",
      "Epoch 3/110\n",
      " - 72s - loss: 0.1563 - acc: 0.9623 - val_loss: 0.2110 - val_acc: 0.9600\n",
      "Epoch 4/110\n",
      " - 72s - loss: 0.1353 - acc: 0.9648 - val_loss: 0.1476 - val_acc: 0.9631\n",
      "Epoch 5/110\n",
      " - 72s - loss: 0.1261 - acc: 0.9658 - val_loss: 0.1216 - val_acc: 0.9654\n",
      "Epoch 6/110\n",
      " - 72s - loss: 0.1230 - acc: 0.9659 - val_loss: 0.1162 - val_acc: 0.9648\n",
      "Epoch 7/110\n",
      " - 71s - loss: 0.1216 - acc: 0.9658 - val_loss: 0.1121 - val_acc: 0.9656\n",
      "Epoch 8/110\n",
      " - 71s - loss: 0.1235 - acc: 0.9655 - val_loss: 0.1069 - val_acc: 0.9659\n",
      "Epoch 9/110\n",
      " - 71s - loss: 0.1127 - acc: 0.9667 - val_loss: 0.1036 - val_acc: 0.9661\n",
      "Epoch 10/110\n",
      " - 71s - loss: 0.1118 - acc: 0.9666 - val_loss: 0.1059 - val_acc: 0.9651\n",
      "Epoch 11/110\n",
      " - 71s - loss: 0.1124 - acc: 0.9665 - val_loss: 0.1013 - val_acc: 0.9662\n",
      "Epoch 12/110\n",
      " - 73s - loss: 0.1078 - acc: 0.9669 - val_loss: 0.1004 - val_acc: 0.9662\n",
      "Epoch 13/110\n",
      " - 72s - loss: 0.1093 - acc: 0.9666 - val_loss: 0.1006 - val_acc: 0.9660\n",
      "Epoch 14/110\n",
      " - 72s - loss: 0.1056 - acc: 0.9669 - val_loss: 0.0982 - val_acc: 0.9663\n",
      "Epoch 15/110\n",
      " - 72s - loss: 0.1062 - acc: 0.9668 - val_loss: 0.0971 - val_acc: 0.9664\n",
      "Epoch 16/110\n",
      " - 71s - loss: 0.1010 - acc: 0.9673 - val_loss: 0.0972 - val_acc: 0.9662\n",
      "Epoch 17/110\n",
      " - 71s - loss: 0.1000 - acc: 0.9673 - val_loss: 0.0999 - val_acc: 0.9656\n",
      "Epoch 18/110\n",
      " - 72s - loss: 0.1007 - acc: 0.9673 - val_loss: 0.1012 - val_acc: 0.9654\n",
      "Epoch 19/110\n",
      " - 71s - loss: 0.1006 - acc: 0.9672 - val_loss: 0.0969 - val_acc: 0.9660\n",
      "Epoch 20/110\n",
      " - 71s - loss: 0.0982 - acc: 0.9673 - val_loss: 0.0942 - val_acc: 0.9664\n",
      "Epoch 21/110\n",
      " - 71s - loss: 0.0989 - acc: 0.9672 - val_loss: 0.0930 - val_acc: 0.9666\n",
      "Epoch 22/110\n",
      " - 72s - loss: 0.0976 - acc: 0.9672 - val_loss: 0.0933 - val_acc: 0.9666\n",
      "Epoch 23/110\n",
      " - 71s - loss: 0.0975 - acc: 0.9673 - val_loss: 0.1042 - val_acc: 0.9635\n",
      "Epoch 24/110\n",
      " - 71s - loss: 0.0969 - acc: 0.9673 - val_loss: 0.0934 - val_acc: 0.9667\n",
      "Epoch 25/110\n",
      " - 71s - loss: 0.0970 - acc: 0.9673 - val_loss: 0.0934 - val_acc: 0.9665\n",
      "Epoch 26/110\n",
      " - 72s - loss: 0.0960 - acc: 0.9674 - val_loss: 0.0919 - val_acc: 0.9668\n",
      "Epoch 27/110\n",
      " - 72s - loss: 0.0948 - acc: 0.9675 - val_loss: 0.0937 - val_acc: 0.9662\n",
      "Epoch 28/110\n",
      " - 71s - loss: 0.0939 - acc: 0.9675 - val_loss: 0.0928 - val_acc: 0.9665\n",
      "Epoch 29/110\n",
      " - 71s - loss: 0.0935 - acc: 0.9676 - val_loss: 0.0909 - val_acc: 0.9666\n",
      "Epoch 30/110\n",
      " - 72s - loss: 0.0948 - acc: 0.9674 - val_loss: 0.0942 - val_acc: 0.9658\n",
      "Epoch 31/110\n",
      " - 72s - loss: 0.0923 - acc: 0.9679 - val_loss: 0.0898 - val_acc: 0.9668\n",
      "Epoch 32/110\n",
      " - 71s - loss: 0.0913 - acc: 0.9678 - val_loss: 0.0891 - val_acc: 0.9669\n",
      "Epoch 33/110\n",
      " - 71s - loss: 0.0910 - acc: 0.9679 - val_loss: 0.0902 - val_acc: 0.9666\n",
      "Epoch 34/110\n",
      " - 71s - loss: 0.0900 - acc: 0.9680 - val_loss: 0.0886 - val_acc: 0.9670\n",
      "Epoch 35/110\n",
      " - 71s - loss: 0.0899 - acc: 0.9680 - val_loss: 0.0885 - val_acc: 0.9669\n",
      "Epoch 36/110\n",
      " - 72s - loss: 0.0897 - acc: 0.9681 - val_loss: 0.0887 - val_acc: 0.9669\n",
      "Epoch 37/110\n",
      " - 71s - loss: 0.0896 - acc: 0.9680 - val_loss: 0.0894 - val_acc: 0.9666\n",
      "Epoch 38/110\n",
      " - 72s - loss: 0.0891 - acc: 0.9682 - val_loss: 0.0881 - val_acc: 0.9669\n",
      "Epoch 39/110\n",
      " - 71s - loss: 0.0901 - acc: 0.9680 - val_loss: 0.0889 - val_acc: 0.9669\n",
      "Epoch 40/110\n",
      " - 71s - loss: 0.0894 - acc: 0.9680 - val_loss: 0.0884 - val_acc: 0.9669\n",
      "Epoch 41/110\n",
      " - 71s - loss: 0.0888 - acc: 0.9681 - val_loss: 0.0882 - val_acc: 0.9669\n",
      "Epoch 42/110\n",
      " - 71s - loss: 0.0891 - acc: 0.9681 - val_loss: 0.0882 - val_acc: 0.9669\n",
      "Epoch 43/110\n",
      " - 72s - loss: 0.0895 - acc: 0.9679 - val_loss: 0.0882 - val_acc: 0.9669\n",
      "Epoch 44/110\n",
      " - 71s - loss: 0.0882 - acc: 0.9682 - val_loss: 0.0878 - val_acc: 0.9669\n",
      "Epoch 45/110\n",
      " - 72s - loss: 0.0882 - acc: 0.9682 - val_loss: 0.0879 - val_acc: 0.9669\n",
      "Epoch 46/110\n",
      " - 71s - loss: 0.0887 - acc: 0.9681 - val_loss: 0.0876 - val_acc: 0.9671\n",
      "Epoch 47/110\n",
      " - 71s - loss: 0.0882 - acc: 0.9682 - val_loss: 0.0880 - val_acc: 0.9669\n",
      "Epoch 48/110\n",
      " - 73s - loss: 0.0881 - acc: 0.9682 - val_loss: 0.0878 - val_acc: 0.9669\n",
      "Epoch 49/110\n",
      " - 71s - loss: 0.0876 - acc: 0.9682 - val_loss: 0.0878 - val_acc: 0.9670\n",
      "Epoch 50/110\n",
      " - 71s - loss: 0.0874 - acc: 0.9683 - val_loss: 0.0880 - val_acc: 0.9668\n",
      "Epoch 51/110\n",
      " - 71s - loss: 0.0877 - acc: 0.9682 - val_loss: 0.0875 - val_acc: 0.9669\n",
      "Epoch 52/110\n",
      " - 71s - loss: 0.0873 - acc: 0.9683 - val_loss: 0.0882 - val_acc: 0.9667\n",
      "Epoch 53/110\n",
      " - 72s - loss: 0.0884 - acc: 0.9682 - val_loss: 0.0874 - val_acc: 0.9670\n",
      "Epoch 54/110\n",
      " - 71s - loss: 0.0874 - acc: 0.9683 - val_loss: 0.0885 - val_acc: 0.9667\n",
      "Epoch 55/110\n",
      " - 71s - loss: 0.0873 - acc: 0.9683 - val_loss: 0.0880 - val_acc: 0.9667\n",
      "Epoch 56/110\n",
      " - 71s - loss: 0.0870 - acc: 0.9683 - val_loss: 0.0873 - val_acc: 0.9669\n",
      "Epoch 57/110\n",
      " - 71s - loss: 0.0878 - acc: 0.9682 - val_loss: 0.0890 - val_acc: 0.9668\n",
      "Epoch 58/110\n",
      " - 71s - loss: 0.0887 - acc: 0.9682 - val_loss: 0.0882 - val_acc: 0.9668\n",
      "Epoch 59/110\n",
      " - 72s - loss: 0.0872 - acc: 0.9684 - val_loss: 0.0873 - val_acc: 0.9670\n",
      "Epoch 60/110\n",
      " - 72s - loss: 0.0871 - acc: 0.9683 - val_loss: 0.0872 - val_acc: 0.9670\n",
      "Epoch 61/110\n",
      " - 71s - loss: 0.0868 - acc: 0.9684 - val_loss: 0.0870 - val_acc: 0.9671\n",
      "Epoch 62/110\n",
      " - 72s - loss: 0.0861 - acc: 0.9685 - val_loss: 0.0869 - val_acc: 0.9671\n",
      "Epoch 63/110\n",
      " - 71s - loss: 0.0861 - acc: 0.9685 - val_loss: 0.0875 - val_acc: 0.9668\n",
      "Epoch 64/110\n",
      " - 71s - loss: 0.0863 - acc: 0.9684 - val_loss: 0.0870 - val_acc: 0.9670\n",
      "Epoch 65/110\n",
      " - 71s - loss: 0.0866 - acc: 0.9683 - val_loss: 0.0870 - val_acc: 0.9670\n",
      "Epoch 66/110\n",
      " - 71s - loss: 0.0861 - acc: 0.9685 - val_loss: 0.0873 - val_acc: 0.9669\n",
      "Epoch 67/110\n",
      " - 72s - loss: 0.0858 - acc: 0.9685 - val_loss: 0.0872 - val_acc: 0.9670\n",
      "Epoch 68/110\n",
      " - 72s - loss: 0.0862 - acc: 0.9685 - val_loss: 0.0869 - val_acc: 0.9670\n",
      "Epoch 69/110\n",
      " - 72s - loss: 0.0862 - acc: 0.9685 - val_loss: 0.0873 - val_acc: 0.9668\n",
      "Epoch 70/110\n",
      " - 71s - loss: 0.0864 - acc: 0.9685 - val_loss: 0.0871 - val_acc: 0.9669\n",
      "Epoch 71/110\n",
      " - 72s - loss: 0.0866 - acc: 0.9685 - val_loss: 0.0870 - val_acc: 0.9670\n",
      "Epoch 72/110\n",
      " - 71s - loss: 0.0857 - acc: 0.9686 - val_loss: 0.0867 - val_acc: 0.9671\n",
      "Epoch 73/110\n",
      " - 71s - loss: 0.0857 - acc: 0.9686 - val_loss: 0.0868 - val_acc: 0.9670\n",
      "Epoch 74/110\n",
      " - 71s - loss: 0.0856 - acc: 0.9686 - val_loss: 0.0867 - val_acc: 0.9670\n",
      "Epoch 75/110\n",
      " - 71s - loss: 0.0859 - acc: 0.9686 - val_loss: 0.0868 - val_acc: 0.9670\n",
      "Epoch 76/110\n",
      " - 71s - loss: 0.0855 - acc: 0.9686 - val_loss: 0.0869 - val_acc: 0.9670\n",
      "Epoch 77/110\n",
      " - 71s - loss: 0.0850 - acc: 0.9687 - val_loss: 0.0870 - val_acc: 0.9668\n",
      "Epoch 78/110\n",
      " - 71s - loss: 0.0856 - acc: 0.9686 - val_loss: 0.0866 - val_acc: 0.9671\n",
      "Epoch 79/110\n",
      " - 71s - loss: 0.0850 - acc: 0.9686 - val_loss: 0.0866 - val_acc: 0.9670\n",
      "Epoch 80/110\n",
      " - 71s - loss: 0.0851 - acc: 0.9687 - val_loss: 0.0865 - val_acc: 0.9670\n",
      "Epoch 81/110\n",
      " - 71s - loss: 0.0852 - acc: 0.9686 - val_loss: 0.0869 - val_acc: 0.9669\n",
      "Epoch 82/110\n",
      " - 71s - loss: 0.0849 - acc: 0.9687 - val_loss: 0.0868 - val_acc: 0.9669\n",
      "Epoch 83/110\n",
      " - 71s - loss: 0.0847 - acc: 0.9687 - val_loss: 0.0864 - val_acc: 0.9671\n",
      "Epoch 84/110\n",
      " - 71s - loss: 0.0852 - acc: 0.9686 - val_loss: 0.0867 - val_acc: 0.9670\n",
      "Epoch 85/110\n",
      " - 71s - loss: 0.0848 - acc: 0.9688 - val_loss: 0.0865 - val_acc: 0.9670\n",
      "Epoch 86/110\n",
      " - 72s - loss: 0.0844 - acc: 0.9688 - val_loss: 0.0866 - val_acc: 0.9670\n",
      "Epoch 87/110\n",
      " - 71s - loss: 0.0845 - acc: 0.9688 - val_loss: 0.0865 - val_acc: 0.9670\n",
      "Epoch 88/110\n",
      " - 71s - loss: 0.0849 - acc: 0.9687 - val_loss: 0.0869 - val_acc: 0.9670\n",
      "Epoch 89/110\n",
      " - 71s - loss: 0.0843 - acc: 0.9689 - val_loss: 0.0868 - val_acc: 0.9669\n",
      "Epoch 90/110\n",
      " - 72s - loss: 0.0845 - acc: 0.9687 - val_loss: 0.0865 - val_acc: 0.9671\n",
      "Epoch 91/110\n",
      " - 72s - loss: 0.0845 - acc: 0.9688 - val_loss: 0.0863 - val_acc: 0.9671\n",
      "Epoch 92/110\n",
      " - 72s - loss: 0.0840 - acc: 0.9689 - val_loss: 0.0863 - val_acc: 0.9671\n",
      "Epoch 93/110\n",
      " - 72s - loss: 0.0838 - acc: 0.9690 - val_loss: 0.0862 - val_acc: 0.9671\n",
      "Epoch 94/110\n",
      " - 72s - loss: 0.0837 - acc: 0.9690 - val_loss: 0.0862 - val_acc: 0.9671\n",
      "Epoch 95/110\n",
      " - 71s - loss: 0.0839 - acc: 0.9689 - val_loss: 0.0862 - val_acc: 0.9671\n",
      "Epoch 96/110\n",
      " - 71s - loss: 0.0845 - acc: 0.9689 - val_loss: 0.0863 - val_acc: 0.9671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/110\n",
      " - 71s - loss: 0.0842 - acc: 0.9688 - val_loss: 0.0863 - val_acc: 0.9671\n",
      "Epoch 98/110\n",
      " - 71s - loss: 0.0834 - acc: 0.9690 - val_loss: 0.0862 - val_acc: 0.9671\n",
      "Epoch 99/110\n",
      " - 71s - loss: 0.0837 - acc: 0.9690 - val_loss: 0.0862 - val_acc: 0.9671\n",
      "Epoch 100/110\n",
      " - 72s - loss: 0.0841 - acc: 0.9689 - val_loss: 0.0863 - val_acc: 0.9671\n",
      "Epoch 101/110\n",
      " - 71s - loss: 0.0840 - acc: 0.9689 - val_loss: 0.0862 - val_acc: 0.9671\n",
      "Epoch 102/110\n",
      " - 71s - loss: 0.0837 - acc: 0.9690 - val_loss: 0.0862 - val_acc: 0.9671\n",
      "Epoch 103/110\n",
      " - 71s - loss: 0.0843 - acc: 0.9688 - val_loss: 0.0863 - val_acc: 0.9671\n",
      "Epoch 104/110\n",
      " - 71s - loss: 0.0843 - acc: 0.9688 - val_loss: 0.0863 - val_acc: 0.9671\n",
      "Epoch 105/110\n",
      " - 71s - loss: 0.0839 - acc: 0.9690 - val_loss: 0.0863 - val_acc: 0.9670\n",
      "Epoch 106/110\n",
      " - 72s - loss: 0.0837 - acc: 0.9689 - val_loss: 0.0863 - val_acc: 0.9671\n",
      "Epoch 107/110\n",
      " - 72s - loss: 0.0838 - acc: 0.9689 - val_loss: 0.0862 - val_acc: 0.9671\n",
      "Epoch 108/110\n",
      " - 71s - loss: 0.0848 - acc: 0.9689 - val_loss: 0.0862 - val_acc: 0.9671\n",
      "Epoch 109/110\n",
      " - 71s - loss: 0.0842 - acc: 0.9688 - val_loss: 0.0862 - val_acc: 0.9671\n",
      "Epoch 110/110\n",
      " - 71s - loss: 0.0838 - acc: 0.9689 - val_loss: 0.0862 - val_acc: 0.9671\n",
      "Training fold 1 completed. macro f1 score : 0.93847\n",
      "Our training dataset shape is (1000, 4000, 19)\n",
      "Our validation dataset shape is (250, 4000, 19)\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2842 of 3912 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 108s - loss: 0.5576 - acc: 0.8447 - val_loss: 1.0067 - val_acc: 0.8901\n",
      "Epoch 2/110\n",
      " - 71s - loss: 0.2092 - acc: 0.9534 - val_loss: 0.5351 - val_acc: 0.9416\n",
      "Epoch 3/110\n",
      " - 71s - loss: 0.1529 - acc: 0.9627 - val_loss: 0.2555 - val_acc: 0.9620\n",
      "Epoch 4/110\n",
      " - 73s - loss: 0.1388 - acc: 0.9640 - val_loss: 0.1596 - val_acc: 0.9667\n",
      "Epoch 5/110\n",
      " - 73s - loss: 0.1339 - acc: 0.9648 - val_loss: 0.1135 - val_acc: 0.9687\n",
      "Epoch 6/110\n",
      " - 71s - loss: 0.1270 - acc: 0.9652 - val_loss: 0.1098 - val_acc: 0.9680\n",
      "Epoch 7/110\n",
      " - 71s - loss: 0.1236 - acc: 0.9654 - val_loss: 0.0998 - val_acc: 0.9690\n",
      "Epoch 8/110\n",
      " - 71s - loss: 0.1180 - acc: 0.9659 - val_loss: 0.0996 - val_acc: 0.9689\n",
      "Epoch 9/110\n",
      " - 71s - loss: 0.1163 - acc: 0.9658 - val_loss: 0.0960 - val_acc: 0.9692\n",
      "Epoch 10/110\n",
      " - 71s - loss: 0.1137 - acc: 0.9660 - val_loss: 0.0941 - val_acc: 0.9692\n",
      "Epoch 11/110\n",
      " - 73s - loss: 0.1116 - acc: 0.9661 - val_loss: 0.0932 - val_acc: 0.9693\n",
      "Epoch 12/110\n",
      " - 72s - loss: 0.1086 - acc: 0.9663 - val_loss: 0.0913 - val_acc: 0.9693\n",
      "Epoch 13/110\n",
      " - 72s - loss: 0.1062 - acc: 0.9664 - val_loss: 0.0959 - val_acc: 0.9682\n",
      "Epoch 14/110\n",
      " - 71s - loss: 0.1086 - acc: 0.9661 - val_loss: 0.0931 - val_acc: 0.9690\n",
      "Epoch 15/110\n",
      " - 72s - loss: 0.1089 - acc: 0.9661 - val_loss: 0.0910 - val_acc: 0.9692\n",
      "Epoch 16/110\n",
      " - 71s - loss: 0.1052 - acc: 0.9663 - val_loss: 0.0911 - val_acc: 0.9692\n",
      "Epoch 17/110\n",
      " - 72s - loss: 0.1051 - acc: 0.9664 - val_loss: 0.0904 - val_acc: 0.9689\n",
      "Epoch 18/110\n",
      " - 72s - loss: 0.1026 - acc: 0.9664 - val_loss: 0.0903 - val_acc: 0.9691\n",
      "Epoch 19/110\n",
      " - 71s - loss: 0.1021 - acc: 0.9664 - val_loss: 0.0870 - val_acc: 0.9695\n",
      "Epoch 20/110\n",
      " - 71s - loss: 0.1012 - acc: 0.9664 - val_loss: 0.0868 - val_acc: 0.9695\n",
      "Epoch 21/110\n",
      " - 71s - loss: 0.0995 - acc: 0.9666 - val_loss: 0.0888 - val_acc: 0.9689\n",
      "Epoch 22/110\n",
      " - 71s - loss: 0.1003 - acc: 0.9664 - val_loss: 0.0877 - val_acc: 0.9693\n",
      "Epoch 23/110\n",
      " - 71s - loss: 0.1025 - acc: 0.9663 - val_loss: 0.0912 - val_acc: 0.9683\n",
      "Epoch 24/110\n",
      " - 71s - loss: 0.0995 - acc: 0.9666 - val_loss: 0.0867 - val_acc: 0.9695\n",
      "Epoch 25/110\n",
      " - 71s - loss: 0.1047 - acc: 0.9660 - val_loss: 0.0908 - val_acc: 0.9687\n",
      "Epoch 26/110\n",
      " - 71s - loss: 0.0989 - acc: 0.9666 - val_loss: 0.0852 - val_acc: 0.9695\n",
      "Epoch 27/110\n",
      " - 71s - loss: 0.1006 - acc: 0.9663 - val_loss: 0.0882 - val_acc: 0.9689\n",
      "Epoch 28/110\n",
      " - 71s - loss: 0.0969 - acc: 0.9668 - val_loss: 0.0868 - val_acc: 0.9691\n",
      "Epoch 29/110\n",
      " - 71s - loss: 0.0950 - acc: 0.9669 - val_loss: 0.0858 - val_acc: 0.9692\n",
      "Epoch 30/110\n",
      " - 71s - loss: 0.0958 - acc: 0.9667 - val_loss: 0.0862 - val_acc: 0.9691\n",
      "Epoch 31/110\n",
      " - 71s - loss: 0.0933 - acc: 0.9672 - val_loss: 0.0828 - val_acc: 0.9697\n",
      "Epoch 32/110\n",
      " - 71s - loss: 0.0925 - acc: 0.9673 - val_loss: 0.0824 - val_acc: 0.9697\n",
      "Epoch 33/110\n",
      " - 71s - loss: 0.0921 - acc: 0.9673 - val_loss: 0.0828 - val_acc: 0.9696\n",
      "Epoch 34/110\n",
      " - 71s - loss: 0.0921 - acc: 0.9672 - val_loss: 0.0824 - val_acc: 0.9697\n",
      "Epoch 35/110\n",
      " - 71s - loss: 0.0919 - acc: 0.9673 - val_loss: 0.0821 - val_acc: 0.9697\n",
      "Epoch 36/110\n",
      " - 71s - loss: 0.1001 - acc: 0.9657 - val_loss: 0.0961 - val_acc: 0.9673\n",
      "Epoch 37/110\n",
      " - 71s - loss: 0.0968 - acc: 0.9666 - val_loss: 0.0843 - val_acc: 0.9694\n",
      "Epoch 38/110\n",
      " - 71s - loss: 0.0933 - acc: 0.9671 - val_loss: 0.0826 - val_acc: 0.9697\n",
      "Epoch 39/110\n",
      " - 71s - loss: 0.0931 - acc: 0.9671 - val_loss: 0.0833 - val_acc: 0.9695\n",
      "Epoch 40/110\n",
      " - 71s - loss: 0.0933 - acc: 0.9671 - val_loss: 0.0838 - val_acc: 0.9693\n",
      "Epoch 41/110\n",
      " - 71s - loss: 0.0917 - acc: 0.9672 - val_loss: 0.0821 - val_acc: 0.9697\n",
      "Epoch 42/110\n",
      " - 71s - loss: 0.0922 - acc: 0.9673 - val_loss: 0.0818 - val_acc: 0.9697\n",
      "Epoch 43/110\n",
      " - 71s - loss: 0.0914 - acc: 0.9674 - val_loss: 0.0824 - val_acc: 0.9696\n",
      "Epoch 44/110\n",
      " - 71s - loss: 0.0909 - acc: 0.9673 - val_loss: 0.0815 - val_acc: 0.9697\n",
      "Epoch 45/110\n",
      " - 71s - loss: 0.0904 - acc: 0.9674 - val_loss: 0.0816 - val_acc: 0.9697\n",
      "Epoch 46/110\n",
      " - 71s - loss: 0.0905 - acc: 0.9675 - val_loss: 0.0814 - val_acc: 0.9697\n",
      "Epoch 47/110\n",
      " - 71s - loss: 0.0904 - acc: 0.9674 - val_loss: 0.0813 - val_acc: 0.9698\n",
      "Epoch 48/110\n",
      " - 71s - loss: 0.0899 - acc: 0.9675 - val_loss: 0.0811 - val_acc: 0.9698\n",
      "Epoch 49/110\n",
      " - 71s - loss: 0.0894 - acc: 0.9676 - val_loss: 0.0813 - val_acc: 0.9697\n",
      "Epoch 50/110\n",
      " - 75s - loss: 0.0900 - acc: 0.9674 - val_loss: 0.0813 - val_acc: 0.9698\n",
      "Epoch 51/110\n",
      " - 71s - loss: 0.0903 - acc: 0.9674 - val_loss: 0.0814 - val_acc: 0.9697\n",
      "Epoch 52/110\n",
      " - 71s - loss: 0.0897 - acc: 0.9675 - val_loss: 0.0811 - val_acc: 0.9697\n",
      "Epoch 53/110\n",
      " - 71s - loss: 0.0895 - acc: 0.9675 - val_loss: 0.0810 - val_acc: 0.9697\n",
      "Epoch 54/110\n",
      " - 71s - loss: 0.0895 - acc: 0.9675 - val_loss: 0.0817 - val_acc: 0.9696\n",
      "Epoch 55/110\n",
      " - 71s - loss: 0.0889 - acc: 0.9676 - val_loss: 0.0810 - val_acc: 0.9698\n",
      "Epoch 56/110\n",
      " - 71s - loss: 0.0895 - acc: 0.9676 - val_loss: 0.0809 - val_acc: 0.9697\n",
      "Epoch 57/110\n",
      " - 73s - loss: 0.0890 - acc: 0.9676 - val_loss: 0.0807 - val_acc: 0.9697\n",
      "Epoch 58/110\n",
      " - 71s - loss: 0.0890 - acc: 0.9675 - val_loss: 0.0812 - val_acc: 0.9697\n",
      "Epoch 59/110\n",
      " - 71s - loss: 0.0889 - acc: 0.9676 - val_loss: 0.0808 - val_acc: 0.9697\n",
      "Epoch 60/110\n",
      " - 71s - loss: 0.0886 - acc: 0.9676 - val_loss: 0.0809 - val_acc: 0.9698\n",
      "Epoch 61/110\n",
      " - 71s - loss: 0.0883 - acc: 0.9677 - val_loss: 0.0805 - val_acc: 0.9698\n",
      "Epoch 62/110\n",
      " - 80s - loss: 0.0886 - acc: 0.9675 - val_loss: 0.0806 - val_acc: 0.9698\n",
      "Epoch 63/110\n",
      " - 80s - loss: 0.0888 - acc: 0.9676 - val_loss: 0.0805 - val_acc: 0.9698\n",
      "Epoch 64/110\n",
      " - 80s - loss: 0.0894 - acc: 0.9675 - val_loss: 0.0808 - val_acc: 0.9698\n",
      "Epoch 65/110\n",
      " - 80s - loss: 0.0884 - acc: 0.9677 - val_loss: 0.0807 - val_acc: 0.9698\n",
      "Epoch 66/110\n",
      " - 86s - loss: 0.0887 - acc: 0.9677 - val_loss: 0.0809 - val_acc: 0.9698\n",
      "Epoch 67/110\n",
      " - 77s - loss: 0.0883 - acc: 0.9677 - val_loss: 0.0805 - val_acc: 0.9698\n",
      "Epoch 68/110\n",
      " - 70s - loss: 0.0882 - acc: 0.9676 - val_loss: 0.0805 - val_acc: 0.9698\n",
      "Epoch 69/110\n",
      " - 70s - loss: 0.0874 - acc: 0.9678 - val_loss: 0.0806 - val_acc: 0.9698\n",
      "Epoch 70/110\n",
      " - 70s - loss: 0.0881 - acc: 0.9677 - val_loss: 0.0805 - val_acc: 0.9698\n",
      "Epoch 71/110\n",
      " - 70s - loss: 0.0882 - acc: 0.9678 - val_loss: 0.0804 - val_acc: 0.9698\n",
      "Epoch 72/110\n",
      " - 70s - loss: 0.0876 - acc: 0.9678 - val_loss: 0.0804 - val_acc: 0.9698\n",
      "Epoch 73/110\n",
      " - 70s - loss: 0.0880 - acc: 0.9678 - val_loss: 0.0811 - val_acc: 0.9696\n",
      "Epoch 74/110\n",
      " - 70s - loss: 0.0877 - acc: 0.9677 - val_loss: 0.0809 - val_acc: 0.9697\n",
      "Epoch 75/110\n",
      " - 70s - loss: 0.0874 - acc: 0.9678 - val_loss: 0.0804 - val_acc: 0.9698\n",
      "Epoch 76/110\n",
      " - 70s - loss: 0.0877 - acc: 0.9678 - val_loss: 0.0803 - val_acc: 0.9698\n",
      "Epoch 77/110\n",
      " - 70s - loss: 0.0881 - acc: 0.9677 - val_loss: 0.0805 - val_acc: 0.9698\n",
      "Epoch 78/110\n",
      " - 70s - loss: 0.0872 - acc: 0.9679 - val_loss: 0.0803 - val_acc: 0.9698\n",
      "Epoch 79/110\n",
      " - 76s - loss: 0.0868 - acc: 0.9679 - val_loss: 0.0804 - val_acc: 0.9698\n",
      "Epoch 80/110\n",
      " - 80s - loss: 0.0869 - acc: 0.9679 - val_loss: 0.0803 - val_acc: 0.9698\n",
      "Epoch 81/110\n",
      " - 82s - loss: 0.0872 - acc: 0.9679 - val_loss: 0.0801 - val_acc: 0.9698\n",
      "Epoch 82/110\n",
      " - 73s - loss: 0.0869 - acc: 0.9679 - val_loss: 0.0803 - val_acc: 0.9698\n",
      "Epoch 83/110\n",
      " - 74s - loss: 0.0868 - acc: 0.9680 - val_loss: 0.0803 - val_acc: 0.9697\n",
      "Epoch 84/110\n",
      " - 72s - loss: 0.0876 - acc: 0.9677 - val_loss: 0.0809 - val_acc: 0.9696\n",
      "Epoch 85/110\n",
      " - 74s - loss: 0.0870 - acc: 0.9679 - val_loss: 0.0801 - val_acc: 0.9699\n",
      "Epoch 86/110\n",
      " - 73s - loss: 0.0869 - acc: 0.9678 - val_loss: 0.0801 - val_acc: 0.9698\n",
      "Epoch 87/110\n",
      " - 72s - loss: 0.0866 - acc: 0.9680 - val_loss: 0.0803 - val_acc: 0.9697\n",
      "Epoch 88/110\n",
      " - 73s - loss: 0.0864 - acc: 0.9680 - val_loss: 0.0802 - val_acc: 0.9697\n",
      "Epoch 89/110\n",
      " - 72s - loss: 0.0867 - acc: 0.9680 - val_loss: 0.0800 - val_acc: 0.9698\n",
      "Epoch 90/110\n",
      " - 74s - loss: 0.0873 - acc: 0.9679 - val_loss: 0.0804 - val_acc: 0.9698\n",
      "Epoch 91/110\n",
      " - 73s - loss: 0.0862 - acc: 0.9681 - val_loss: 0.0799 - val_acc: 0.9698\n",
      "Epoch 92/110\n",
      " - 72s - loss: 0.0859 - acc: 0.9681 - val_loss: 0.0799 - val_acc: 0.9699\n",
      "Epoch 93/110\n",
      " - 72s - loss: 0.0860 - acc: 0.9681 - val_loss: 0.0798 - val_acc: 0.9699\n",
      "Epoch 94/110\n",
      " - 73s - loss: 0.0855 - acc: 0.9682 - val_loss: 0.0798 - val_acc: 0.9699\n",
      "Epoch 95/110\n",
      " - 72s - loss: 0.0863 - acc: 0.9681 - val_loss: 0.0799 - val_acc: 0.9698\n",
      "Epoch 96/110\n",
      " - 72s - loss: 0.0862 - acc: 0.9681 - val_loss: 0.0798 - val_acc: 0.9699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/110\n",
      " - 72s - loss: 0.0861 - acc: 0.9682 - val_loss: 0.0798 - val_acc: 0.9699\n",
      "Epoch 98/110\n",
      " - 73s - loss: 0.0858 - acc: 0.9682 - val_loss: 0.0798 - val_acc: 0.9698\n",
      "Epoch 99/110\n",
      " - 73s - loss: 0.0868 - acc: 0.9680 - val_loss: 0.0798 - val_acc: 0.9698\n",
      "Epoch 100/110\n",
      " - 74s - loss: 0.0865 - acc: 0.9681 - val_loss: 0.0798 - val_acc: 0.9699\n",
      "Epoch 101/110\n",
      " - 72s - loss: 0.0861 - acc: 0.9681 - val_loss: 0.0798 - val_acc: 0.9699\n",
      "Epoch 102/110\n",
      " - 73s - loss: 0.0860 - acc: 0.9682 - val_loss: 0.0798 - val_acc: 0.9699\n",
      "Epoch 103/110\n",
      " - 73s - loss: 0.0857 - acc: 0.9682 - val_loss: 0.0798 - val_acc: 0.9699\n",
      "Epoch 104/110\n",
      " - 72s - loss: 0.0858 - acc: 0.9681 - val_loss: 0.0798 - val_acc: 0.9699\n",
      "Epoch 105/110\n",
      " - 73s - loss: 0.0860 - acc: 0.9681 - val_loss: 0.0798 - val_acc: 0.9699\n",
      "Epoch 106/110\n",
      " - 74s - loss: 0.0862 - acc: 0.9682 - val_loss: 0.0798 - val_acc: 0.9699\n",
      "Epoch 107/110\n",
      " - 74s - loss: 0.0862 - acc: 0.9681 - val_loss: 0.0798 - val_acc: 0.9698\n",
      "Epoch 108/110\n",
      " - 72s - loss: 0.0859 - acc: 0.9682 - val_loss: 0.0797 - val_acc: 0.9699\n",
      "Epoch 109/110\n",
      " - 72s - loss: 0.0864 - acc: 0.9681 - val_loss: 0.0798 - val_acc: 0.9699\n",
      "Epoch 110/110\n",
      " - 72s - loss: 0.0875 - acc: 0.9678 - val_loss: 0.0798 - val_acc: 0.9699\n",
      "Training fold 2 completed. macro f1 score : 0.93952\n",
      "Our training dataset shape is (1000, 4000, 19)\n",
      "Our validation dataset shape is (250, 4000, 19)\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 1862 of 3912 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 2902 of 3912 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 109s - loss: 0.5894 - acc: 0.8432 - val_loss: 1.0638 - val_acc: 0.8913\n",
      "Epoch 2/110\n",
      " - 72s - loss: 0.1989 - acc: 0.9556 - val_loss: 0.6040 - val_acc: 0.9363\n",
      "Epoch 3/110\n",
      " - 73s - loss: 0.1548 - acc: 0.9627 - val_loss: 0.2711 - val_acc: 0.9592\n",
      "Epoch 4/110\n",
      " - 72s - loss: 0.1382 - acc: 0.9644 - val_loss: 0.1791 - val_acc: 0.9633\n",
      "Epoch 5/110\n",
      " - 74s - loss: 0.1319 - acc: 0.9650 - val_loss: 0.1252 - val_acc: 0.9665\n",
      "Epoch 6/110\n",
      " - 73s - loss: 0.1246 - acc: 0.9655 - val_loss: 0.1134 - val_acc: 0.9671\n",
      "Epoch 7/110\n",
      " - 72s - loss: 0.1209 - acc: 0.9656 - val_loss: 0.1038 - val_acc: 0.9676\n",
      "Epoch 8/110\n",
      " - 71s - loss: 0.1175 - acc: 0.9659 - val_loss: 0.1040 - val_acc: 0.9676\n",
      "Epoch 9/110\n",
      " - 72s - loss: 0.1180 - acc: 0.9656 - val_loss: 0.1022 - val_acc: 0.9673\n",
      "Epoch 10/110\n",
      " - 72s - loss: 0.1128 - acc: 0.9663 - val_loss: 0.0973 - val_acc: 0.9680\n",
      "Epoch 11/110\n",
      " - 72s - loss: 0.1106 - acc: 0.9663 - val_loss: 0.1065 - val_acc: 0.9654\n",
      "Epoch 12/110\n",
      " - 73s - loss: 0.1108 - acc: 0.9660 - val_loss: 0.0973 - val_acc: 0.9679\n",
      "Epoch 13/110\n",
      " - 73s - loss: 0.1066 - acc: 0.9666 - val_loss: 0.0952 - val_acc: 0.9680\n",
      "Epoch 14/110\n",
      " - 72s - loss: 0.1053 - acc: 0.9665 - val_loss: 0.0942 - val_acc: 0.9679\n",
      "Epoch 15/110\n",
      " - 72s - loss: 0.1033 - acc: 0.9667 - val_loss: 0.0932 - val_acc: 0.9680\n",
      "Epoch 16/110\n",
      " - 72s - loss: 0.1046 - acc: 0.9665 - val_loss: 0.0950 - val_acc: 0.9676\n",
      "Epoch 17/110\n",
      " - 72s - loss: 0.1036 - acc: 0.9666 - val_loss: 0.0920 - val_acc: 0.9681\n",
      "Epoch 18/110\n",
      " - 72s - loss: 0.1009 - acc: 0.9668 - val_loss: 0.0971 - val_acc: 0.9668\n",
      "Epoch 19/110\n",
      " - 74s - loss: 0.1012 - acc: 0.9667 - val_loss: 0.0914 - val_acc: 0.9680\n",
      "Epoch 20/110\n",
      " - 72s - loss: 0.1014 - acc: 0.9666 - val_loss: 0.0927 - val_acc: 0.9680\n",
      "Epoch 21/110\n",
      " - 72s - loss: 0.0989 - acc: 0.9669 - val_loss: 0.0885 - val_acc: 0.9684\n",
      "Epoch 22/110\n",
      " - 72s - loss: 0.0975 - acc: 0.9670 - val_loss: 0.0887 - val_acc: 0.9684\n",
      "Epoch 23/110\n",
      " - 72s - loss: 0.0977 - acc: 0.9670 - val_loss: 0.0890 - val_acc: 0.9682\n",
      "Epoch 24/110\n",
      " - 72s - loss: 0.0980 - acc: 0.9668 - val_loss: 0.0882 - val_acc: 0.9683\n",
      "Epoch 25/110\n",
      " - 72s - loss: 0.0966 - acc: 0.9671 - val_loss: 0.0873 - val_acc: 0.9685\n",
      "Epoch 26/110\n",
      " - 71s - loss: 0.0958 - acc: 0.9671 - val_loss: 0.0889 - val_acc: 0.9683\n",
      "Epoch 27/110\n",
      " - 72s - loss: 0.0970 - acc: 0.9669 - val_loss: 0.0897 - val_acc: 0.9681\n",
      "Epoch 28/110\n",
      " - 72s - loss: 0.0942 - acc: 0.9673 - val_loss: 0.0872 - val_acc: 0.9685\n",
      "Epoch 29/110\n",
      " - 72s - loss: 0.0971 - acc: 0.9668 - val_loss: 0.0886 - val_acc: 0.9684\n",
      "Epoch 30/110\n",
      " - 72s - loss: 0.0964 - acc: 0.9669 - val_loss: 0.0891 - val_acc: 0.9679\n",
      "Epoch 31/110\n",
      " - 71s - loss: 0.0920 - acc: 0.9677 - val_loss: 0.0857 - val_acc: 0.9687\n",
      "Epoch 32/110\n",
      " - 71s - loss: 0.0921 - acc: 0.9676 - val_loss: 0.0850 - val_acc: 0.9688\n",
      "Epoch 33/110\n",
      " - 72s - loss: 0.0911 - acc: 0.9677 - val_loss: 0.0848 - val_acc: 0.9687\n",
      "Epoch 34/110\n",
      " - 72s - loss: 0.0907 - acc: 0.9678 - val_loss: 0.0843 - val_acc: 0.9688\n",
      "Epoch 35/110\n",
      " - 72s - loss: 0.0902 - acc: 0.9678 - val_loss: 0.0845 - val_acc: 0.9688\n",
      "Epoch 36/110\n",
      " - 72s - loss: 0.0904 - acc: 0.9678 - val_loss: 0.0841 - val_acc: 0.9688\n",
      "Epoch 37/110\n",
      " - 72s - loss: 0.0895 - acc: 0.9678 - val_loss: 0.0836 - val_acc: 0.9689\n",
      "Epoch 38/110\n",
      " - 72s - loss: 0.0904 - acc: 0.9677 - val_loss: 0.0846 - val_acc: 0.9687\n",
      "Epoch 39/110\n",
      " - 72s - loss: 0.0901 - acc: 0.9678 - val_loss: 0.0836 - val_acc: 0.9689\n",
      "Epoch 40/110\n",
      " - 72s - loss: 0.0894 - acc: 0.9680 - val_loss: 0.0852 - val_acc: 0.9685\n",
      "Epoch 41/110\n",
      " - 72s - loss: 0.0887 - acc: 0.9681 - val_loss: 0.0830 - val_acc: 0.9691\n",
      "Epoch 42/110\n",
      " - 74s - loss: 0.0884 - acc: 0.9682 - val_loss: 0.0827 - val_acc: 0.9691\n",
      "Epoch 43/110\n",
      " - 72s - loss: 0.0879 - acc: 0.9682 - val_loss: 0.0829 - val_acc: 0.9692\n",
      "Epoch 44/110\n",
      " - 73s - loss: 0.0879 - acc: 0.9683 - val_loss: 0.0827 - val_acc: 0.9691\n",
      "Epoch 45/110\n",
      " - 74s - loss: 0.0881 - acc: 0.9683 - val_loss: 0.0829 - val_acc: 0.9692\n",
      "Epoch 46/110\n",
      " - 72s - loss: 0.0878 - acc: 0.9683 - val_loss: 0.0828 - val_acc: 0.9691\n",
      "Epoch 47/110\n",
      " - 72s - loss: 0.0870 - acc: 0.9684 - val_loss: 0.0823 - val_acc: 0.9693\n",
      "Epoch 48/110\n",
      " - 72s - loss: 0.0877 - acc: 0.9683 - val_loss: 0.0825 - val_acc: 0.9691\n",
      "Epoch 49/110\n",
      " - 73s - loss: 0.0872 - acc: 0.9684 - val_loss: 0.0821 - val_acc: 0.9692\n",
      "Epoch 50/110\n",
      " - 73s - loss: 0.0872 - acc: 0.9683 - val_loss: 0.0826 - val_acc: 0.9692\n",
      "Epoch 51/110\n",
      " - 73s - loss: 0.0869 - acc: 0.9684 - val_loss: 0.0835 - val_acc: 0.9689\n",
      "Epoch 52/110\n",
      " - 72s - loss: 0.0865 - acc: 0.9685 - val_loss: 0.0816 - val_acc: 0.9694\n",
      "Epoch 53/110\n",
      " - 72s - loss: 0.0863 - acc: 0.9686 - val_loss: 0.0822 - val_acc: 0.9693\n",
      "Epoch 54/110\n",
      " - 72s - loss: 0.0860 - acc: 0.9687 - val_loss: 0.0824 - val_acc: 0.9692\n",
      "Epoch 55/110\n",
      " - 72s - loss: 0.0859 - acc: 0.9686 - val_loss: 0.0814 - val_acc: 0.9694\n",
      "Epoch 56/110\n",
      " - 72s - loss: 0.0857 - acc: 0.9687 - val_loss: 0.0817 - val_acc: 0.9694\n",
      "Epoch 57/110\n",
      " - 72s - loss: 0.0864 - acc: 0.9685 - val_loss: 0.0818 - val_acc: 0.9694\n",
      "Epoch 58/110\n",
      " - 72s - loss: 0.0860 - acc: 0.9687 - val_loss: 0.0813 - val_acc: 0.9694\n",
      "Epoch 59/110\n",
      " - 72s - loss: 0.0855 - acc: 0.9688 - val_loss: 0.0810 - val_acc: 0.9696\n",
      "Epoch 60/110\n",
      " - 72s - loss: 0.0852 - acc: 0.9689 - val_loss: 0.0817 - val_acc: 0.9693\n",
      "Epoch 61/110\n",
      " - 73s - loss: 0.0855 - acc: 0.9689 - val_loss: 0.0813 - val_acc: 0.9695\n",
      "Epoch 62/110\n",
      " - 72s - loss: 0.0848 - acc: 0.9689 - val_loss: 0.0807 - val_acc: 0.9696\n",
      "Epoch 63/110\n",
      " - 72s - loss: 0.0849 - acc: 0.9690 - val_loss: 0.0806 - val_acc: 0.9697\n",
      "Epoch 64/110\n",
      " - 72s - loss: 0.0844 - acc: 0.9691 - val_loss: 0.0807 - val_acc: 0.9697\n",
      "Epoch 65/110\n",
      " - 72s - loss: 0.0850 - acc: 0.9690 - val_loss: 0.0808 - val_acc: 0.9697\n",
      "Epoch 66/110\n",
      " - 72s - loss: 0.0843 - acc: 0.9691 - val_loss: 0.0807 - val_acc: 0.9697\n",
      "Epoch 67/110\n",
      " - 72s - loss: 0.0842 - acc: 0.9691 - val_loss: 0.0813 - val_acc: 0.9695\n",
      "Epoch 68/110\n",
      " - 72s - loss: 0.0840 - acc: 0.9693 - val_loss: 0.0802 - val_acc: 0.9698\n",
      "Epoch 69/110\n",
      " - 72s - loss: 0.0840 - acc: 0.9692 - val_loss: 0.0801 - val_acc: 0.9699\n",
      "Epoch 70/110\n",
      " - 73s - loss: 0.0833 - acc: 0.9694 - val_loss: 0.0801 - val_acc: 0.9698\n",
      "Epoch 71/110\n",
      " - 72s - loss: 0.0831 - acc: 0.9694 - val_loss: 0.0799 - val_acc: 0.9699\n",
      "Epoch 72/110\n",
      " - 72s - loss: 0.0838 - acc: 0.9693 - val_loss: 0.0801 - val_acc: 0.9697\n",
      "Epoch 73/110\n",
      " - 72s - loss: 0.0835 - acc: 0.9694 - val_loss: 0.0804 - val_acc: 0.9697\n",
      "Epoch 74/110\n",
      " - 73s - loss: 0.0835 - acc: 0.9693 - val_loss: 0.0797 - val_acc: 0.9700\n",
      "Epoch 75/110\n",
      " - 72s - loss: 0.0833 - acc: 0.9694 - val_loss: 0.0801 - val_acc: 0.9699\n",
      "Epoch 76/110\n",
      " - 72s - loss: 0.0832 - acc: 0.9694 - val_loss: 0.0795 - val_acc: 0.9700\n",
      "Epoch 77/110\n",
      " - 72s - loss: 0.0829 - acc: 0.9695 - val_loss: 0.0795 - val_acc: 0.9701\n",
      "Epoch 78/110\n",
      " - 72s - loss: 0.0827 - acc: 0.9695 - val_loss: 0.0796 - val_acc: 0.9700\n",
      "Epoch 79/110\n",
      " - 72s - loss: 0.0831 - acc: 0.9694 - val_loss: 0.0800 - val_acc: 0.9699\n",
      "Epoch 80/110\n",
      " - 72s - loss: 0.0824 - acc: 0.9696 - val_loss: 0.0797 - val_acc: 0.9701\n",
      "Epoch 81/110\n",
      " - 72s - loss: 0.0822 - acc: 0.9697 - val_loss: 0.0794 - val_acc: 0.9701\n",
      "Epoch 82/110\n",
      " - 72s - loss: 0.0825 - acc: 0.9696 - val_loss: 0.0796 - val_acc: 0.9700\n",
      "Epoch 83/110\n",
      " - 72s - loss: 0.0827 - acc: 0.9696 - val_loss: 0.0803 - val_acc: 0.9697\n",
      "Epoch 84/110\n",
      " - 72s - loss: 0.0822 - acc: 0.9697 - val_loss: 0.0794 - val_acc: 0.9701\n",
      "Epoch 85/110\n",
      " - 72s - loss: 0.0824 - acc: 0.9697 - val_loss: 0.0793 - val_acc: 0.9701\n",
      "Epoch 86/110\n",
      " - 72s - loss: 0.0829 - acc: 0.9695 - val_loss: 0.0796 - val_acc: 0.9700\n",
      "Epoch 87/110\n",
      " - 72s - loss: 0.0818 - acc: 0.9697 - val_loss: 0.0794 - val_acc: 0.9700\n",
      "Epoch 88/110\n",
      " - 72s - loss: 0.0820 - acc: 0.9697 - val_loss: 0.0796 - val_acc: 0.9699\n",
      "Epoch 89/110\n",
      " - 72s - loss: 0.0825 - acc: 0.9697 - val_loss: 0.0799 - val_acc: 0.9699\n",
      "Epoch 90/110\n",
      " - 72s - loss: 0.0821 - acc: 0.9697 - val_loss: 0.0796 - val_acc: 0.9699\n",
      "Epoch 91/110\n",
      " - 72s - loss: 0.0814 - acc: 0.9699 - val_loss: 0.0791 - val_acc: 0.9701\n",
      "Epoch 92/110\n",
      " - 72s - loss: 0.0815 - acc: 0.9698 - val_loss: 0.0790 - val_acc: 0.9702\n",
      "Epoch 93/110\n",
      " - 72s - loss: 0.0818 - acc: 0.9698 - val_loss: 0.0791 - val_acc: 0.9702\n",
      "Epoch 94/110\n",
      " - 72s - loss: 0.0819 - acc: 0.9697 - val_loss: 0.0790 - val_acc: 0.9702\n",
      "Epoch 95/110\n",
      " - 72s - loss: 0.0813 - acc: 0.9699 - val_loss: 0.0791 - val_acc: 0.9701\n",
      "Epoch 96/110\n",
      " - 72s - loss: 0.0815 - acc: 0.9699 - val_loss: 0.0790 - val_acc: 0.9701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/110\n",
      " - 72s - loss: 0.0814 - acc: 0.9699 - val_loss: 0.0792 - val_acc: 0.9700\n",
      "Epoch 98/110\n",
      " - 72s - loss: 0.0812 - acc: 0.9699 - val_loss: 0.0790 - val_acc: 0.9702\n",
      "Epoch 99/110\n",
      " - 72s - loss: 0.0810 - acc: 0.9699 - val_loss: 0.0790 - val_acc: 0.9701\n",
      "Epoch 100/110\n",
      " - 72s - loss: 0.0814 - acc: 0.9699 - val_loss: 0.0790 - val_acc: 0.9701\n",
      "Epoch 101/110\n",
      " - 72s - loss: 0.0815 - acc: 0.9699 - val_loss: 0.0790 - val_acc: 0.9702\n",
      "Epoch 102/110\n",
      " - 72s - loss: 0.0809 - acc: 0.9700 - val_loss: 0.0790 - val_acc: 0.9702\n",
      "Epoch 103/110\n",
      " - 72s - loss: 0.0811 - acc: 0.9699 - val_loss: 0.0789 - val_acc: 0.9702\n",
      "Epoch 104/110\n",
      " - 72s - loss: 0.0812 - acc: 0.9699 - val_loss: 0.0790 - val_acc: 0.9702\n",
      "Epoch 105/110\n",
      " - 72s - loss: 0.0811 - acc: 0.9699 - val_loss: 0.0790 - val_acc: 0.9701\n",
      "Epoch 106/110\n",
      " - 72s - loss: 0.0815 - acc: 0.9698 - val_loss: 0.0790 - val_acc: 0.9702\n",
      "Epoch 107/110\n",
      " - 73s - loss: 0.0812 - acc: 0.9699 - val_loss: 0.0790 - val_acc: 0.9702\n",
      "Epoch 108/110\n",
      " - 73s - loss: 0.0809 - acc: 0.9700 - val_loss: 0.0790 - val_acc: 0.9701\n",
      "Epoch 109/110\n",
      " - 72s - loss: 0.0819 - acc: 0.9698 - val_loss: 0.0790 - val_acc: 0.9702\n",
      "Epoch 110/110\n",
      " - 72s - loss: 0.0811 - acc: 0.9699 - val_loss: 0.0790 - val_acc: 0.9702\n",
      "Training fold 3 completed. macro f1 score : 0.94022\n",
      "Our training dataset shape is (1000, 4000, 19)\n",
      "Our validation dataset shape is (250, 4000, 19)\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2362 of 3912 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 109s - loss: 0.6337 - acc: 0.8284 - val_loss: 1.0166 - val_acc: 0.8824\n",
      "Epoch 2/110\n",
      " - 73s - loss: 0.2148 - acc: 0.9552 - val_loss: 0.6039 - val_acc: 0.9504\n",
      "Epoch 3/110\n",
      " - 72s - loss: 0.1622 - acc: 0.9632 - val_loss: 0.3013 - val_acc: 0.9632\n",
      "Epoch 4/110\n",
      " - 72s - loss: 0.1452 - acc: 0.9647 - val_loss: 0.1749 - val_acc: 0.9654\n",
      "Epoch 5/110\n",
      " - 72s - loss: 0.1449 - acc: 0.9644 - val_loss: 0.1276 - val_acc: 0.9664\n",
      "Epoch 6/110\n",
      " - 72s - loss: 0.1282 - acc: 0.9658 - val_loss: 0.1162 - val_acc: 0.9669\n",
      "Epoch 7/110\n",
      " - 73s - loss: 0.1258 - acc: 0.9658 - val_loss: 0.1111 - val_acc: 0.9670\n",
      "Epoch 8/110\n",
      " - 72s - loss: 0.1213 - acc: 0.9661 - val_loss: 0.1070 - val_acc: 0.9670\n",
      "Epoch 9/110\n",
      " - 72s - loss: 0.1169 - acc: 0.9663 - val_loss: 0.1047 - val_acc: 0.9671\n",
      "Epoch 10/110\n",
      " - 73s - loss: 0.1159 - acc: 0.9662 - val_loss: 0.1038 - val_acc: 0.9664\n",
      "Epoch 11/110\n",
      " - 72s - loss: 0.1133 - acc: 0.9664 - val_loss: 0.0999 - val_acc: 0.9673\n",
      "Epoch 12/110\n",
      " - 72s - loss: 0.1100 - acc: 0.9666 - val_loss: 0.0982 - val_acc: 0.9674\n",
      "Epoch 13/110\n",
      " - 72s - loss: 0.1077 - acc: 0.9667 - val_loss: 0.0963 - val_acc: 0.9673\n",
      "Epoch 14/110\n",
      " - 72s - loss: 0.1059 - acc: 0.9667 - val_loss: 0.0980 - val_acc: 0.9670\n",
      "Epoch 15/110\n",
      " - 73s - loss: 0.1049 - acc: 0.9668 - val_loss: 0.0942 - val_acc: 0.9675\n",
      "Epoch 16/110\n",
      " - 73s - loss: 0.1036 - acc: 0.9667 - val_loss: 0.0948 - val_acc: 0.9673\n",
      "Epoch 17/110\n",
      " - 72s - loss: 0.1035 - acc: 0.9669 - val_loss: 0.0946 - val_acc: 0.9673\n",
      "Epoch 18/110\n",
      " - 72s - loss: 0.1031 - acc: 0.9667 - val_loss: 0.0939 - val_acc: 0.9674\n",
      "Epoch 19/110\n",
      " - 72s - loss: 0.1010 - acc: 0.9669 - val_loss: 0.0938 - val_acc: 0.9672\n",
      "Epoch 20/110\n",
      " - 73s - loss: 0.0999 - acc: 0.9670 - val_loss: 0.0922 - val_acc: 0.9676\n",
      "Epoch 21/110\n",
      " - 73s - loss: 0.1006 - acc: 0.9669 - val_loss: 0.0922 - val_acc: 0.9674\n",
      "Epoch 22/110\n",
      " - 73s - loss: 0.0996 - acc: 0.9670 - val_loss: 0.1029 - val_acc: 0.9648\n",
      "Epoch 23/110\n",
      " - 72s - loss: 0.0995 - acc: 0.9671 - val_loss: 0.0914 - val_acc: 0.9675\n",
      "Epoch 24/110\n",
      " - 72s - loss: 0.0961 - acc: 0.9675 - val_loss: 0.0910 - val_acc: 0.9676\n",
      "Epoch 25/110\n",
      " - 72s - loss: 0.0967 - acc: 0.9673 - val_loss: 0.0967 - val_acc: 0.9663\n",
      "Epoch 26/110\n",
      " - 73s - loss: 0.0951 - acc: 0.9675 - val_loss: 0.0907 - val_acc: 0.9678\n",
      "Epoch 27/110\n",
      " - 72s - loss: 0.0953 - acc: 0.9675 - val_loss: 0.0887 - val_acc: 0.9682\n",
      "Epoch 28/110\n",
      " - 72s - loss: 0.0950 - acc: 0.9676 - val_loss: 0.0882 - val_acc: 0.9682\n",
      "Epoch 29/110\n",
      " - 72s - loss: 0.0937 - acc: 0.9678 - val_loss: 0.0936 - val_acc: 0.9669\n",
      "Epoch 30/110\n",
      " - 74s - loss: 0.0934 - acc: 0.9679 - val_loss: 0.0885 - val_acc: 0.9683\n",
      "Epoch 31/110\n",
      " - 72s - loss: 0.0890 - acc: 0.9686 - val_loss: 0.0852 - val_acc: 0.9688\n",
      "Epoch 32/110\n",
      " - 72s - loss: 0.0893 - acc: 0.9687 - val_loss: 0.0857 - val_acc: 0.9685\n",
      "Epoch 33/110\n",
      " - 72s - loss: 0.0891 - acc: 0.9686 - val_loss: 0.0853 - val_acc: 0.9687\n",
      "Epoch 34/110\n",
      " - 72s - loss: 0.0886 - acc: 0.9687 - val_loss: 0.0853 - val_acc: 0.9687\n",
      "Epoch 35/110\n",
      " - 73s - loss: 0.0883 - acc: 0.9688 - val_loss: 0.0840 - val_acc: 0.9688\n",
      "Epoch 36/110\n",
      " - 72s - loss: 0.0886 - acc: 0.9687 - val_loss: 0.0873 - val_acc: 0.9680\n",
      "Epoch 37/110\n",
      " - 72s - loss: 0.0883 - acc: 0.9688 - val_loss: 0.0851 - val_acc: 0.9687\n",
      "Epoch 38/110\n",
      " - 72s - loss: 0.0870 - acc: 0.9690 - val_loss: 0.0843 - val_acc: 0.9688\n",
      "Epoch 39/110\n",
      " - 72s - loss: 0.0874 - acc: 0.9688 - val_loss: 0.0834 - val_acc: 0.9690\n",
      "Epoch 40/110\n",
      " - 72s - loss: 0.0867 - acc: 0.9689 - val_loss: 0.0836 - val_acc: 0.9690\n",
      "Epoch 41/110\n",
      " - 72s - loss: 0.0862 - acc: 0.9691 - val_loss: 0.0834 - val_acc: 0.9691\n",
      "Epoch 42/110\n",
      " - 72s - loss: 0.0872 - acc: 0.9689 - val_loss: 0.0836 - val_acc: 0.9689\n",
      "Epoch 43/110\n",
      " - 72s - loss: 0.0860 - acc: 0.9692 - val_loss: 0.0831 - val_acc: 0.9691\n",
      "Epoch 44/110\n",
      " - 73s - loss: 0.0855 - acc: 0.9692 - val_loss: 0.0835 - val_acc: 0.9690\n",
      "Epoch 45/110\n",
      " - 72s - loss: 0.0859 - acc: 0.9691 - val_loss: 0.0825 - val_acc: 0.9692\n",
      "Epoch 46/110\n",
      " - 73s - loss: 0.0856 - acc: 0.9692 - val_loss: 0.0830 - val_acc: 0.9691\n",
      "Epoch 47/110\n",
      " - 72s - loss: 0.0862 - acc: 0.9690 - val_loss: 0.0851 - val_acc: 0.9687\n",
      "Epoch 48/110\n",
      " - 72s - loss: 0.0852 - acc: 0.9693 - val_loss: 0.0829 - val_acc: 0.9691\n",
      "Epoch 49/110\n",
      " - 73s - loss: 0.0852 - acc: 0.9692 - val_loss: 0.0829 - val_acc: 0.9691\n",
      "Epoch 50/110\n",
      " - 73s - loss: 0.0850 - acc: 0.9692 - val_loss: 0.0828 - val_acc: 0.9692\n",
      "Epoch 51/110\n",
      " - 73s - loss: 0.0845 - acc: 0.9694 - val_loss: 0.0828 - val_acc: 0.9691\n",
      "Epoch 52/110\n",
      " - 78s - loss: 0.0848 - acc: 0.9694 - val_loss: 0.0825 - val_acc: 0.9691\n",
      "Epoch 53/110\n",
      " - 1426s - loss: 0.0845 - acc: 0.9694 - val_loss: 0.0829 - val_acc: 0.9690\n",
      "Epoch 54/110\n",
      " - 70s - loss: 0.0851 - acc: 0.9692 - val_loss: 0.0824 - val_acc: 0.9693\n",
      "Epoch 55/110\n",
      " - 70s - loss: 0.0842 - acc: 0.9695 - val_loss: 0.0834 - val_acc: 0.9689\n",
      "Epoch 56/110\n",
      " - 70s - loss: 0.0850 - acc: 0.9694 - val_loss: 0.0822 - val_acc: 0.9692\n",
      "Epoch 57/110\n",
      " - 70s - loss: 0.0839 - acc: 0.9695 - val_loss: 0.0831 - val_acc: 0.9688\n",
      "Epoch 58/110\n",
      " - 70s - loss: 0.0839 - acc: 0.9695 - val_loss: 0.0821 - val_acc: 0.9692\n",
      "Epoch 59/110\n",
      " - 70s - loss: 0.0842 - acc: 0.9695 - val_loss: 0.0823 - val_acc: 0.9692\n",
      "Epoch 60/110\n",
      " - 70s - loss: 0.0838 - acc: 0.9696 - val_loss: 0.0821 - val_acc: 0.9692\n",
      "Epoch 61/110\n",
      " - 70s - loss: 0.0837 - acc: 0.9695 - val_loss: 0.0820 - val_acc: 0.9692\n",
      "Epoch 62/110\n",
      " - 70s - loss: 0.0832 - acc: 0.9697 - val_loss: 0.0825 - val_acc: 0.9691\n",
      "Epoch 63/110\n",
      " - 71s - loss: 0.0836 - acc: 0.9695 - val_loss: 0.0818 - val_acc: 0.9693\n",
      "Epoch 64/110\n",
      " - 71s - loss: 0.0842 - acc: 0.9694 - val_loss: 0.0831 - val_acc: 0.9689\n",
      "Epoch 65/110\n",
      " - 70s - loss: 0.0841 - acc: 0.9695 - val_loss: 0.0818 - val_acc: 0.9693\n",
      "Epoch 66/110\n",
      " - 70s - loss: 0.0832 - acc: 0.9696 - val_loss: 0.0820 - val_acc: 0.9691\n",
      "Epoch 67/110\n",
      " - 71s - loss: 0.0836 - acc: 0.9695 - val_loss: 0.0825 - val_acc: 0.9690\n",
      "Epoch 68/110\n",
      " - 70s - loss: 0.0839 - acc: 0.9695 - val_loss: 0.0822 - val_acc: 0.9691\n",
      "Epoch 69/110\n",
      " - 70s - loss: 0.0832 - acc: 0.9696 - val_loss: 0.0823 - val_acc: 0.9691\n",
      "Epoch 70/110\n",
      " - 70s - loss: 0.0831 - acc: 0.9697 - val_loss: 0.0820 - val_acc: 0.9691\n",
      "Epoch 71/110\n",
      " - 70s - loss: 0.0825 - acc: 0.9698 - val_loss: 0.0818 - val_acc: 0.9692\n",
      "Epoch 72/110\n",
      " - 70s - loss: 0.0831 - acc: 0.9697 - val_loss: 0.0823 - val_acc: 0.9691\n",
      "Epoch 73/110\n",
      " - 70s - loss: 0.0826 - acc: 0.9697 - val_loss: 0.0818 - val_acc: 0.9693\n",
      "Epoch 74/110\n",
      " - 70s - loss: 0.0828 - acc: 0.9697 - val_loss: 0.0819 - val_acc: 0.9692\n",
      "Epoch 75/110\n",
      " - 70s - loss: 0.0829 - acc: 0.9697 - val_loss: 0.0820 - val_acc: 0.9692\n",
      "Epoch 76/110\n",
      " - 70s - loss: 0.0828 - acc: 0.9696 - val_loss: 0.0816 - val_acc: 0.9694\n",
      "Epoch 77/110\n",
      " - 70s - loss: 0.0823 - acc: 0.9698 - val_loss: 0.0820 - val_acc: 0.9692\n",
      "Epoch 78/110\n",
      " - 74s - loss: 0.0827 - acc: 0.9697 - val_loss: 0.0816 - val_acc: 0.9692\n",
      "Epoch 79/110\n",
      " - 70s - loss: 0.0823 - acc: 0.9698 - val_loss: 0.0819 - val_acc: 0.9693\n",
      "Epoch 80/110\n",
      " - 70s - loss: 0.0826 - acc: 0.9698 - val_loss: 0.0818 - val_acc: 0.9693\n",
      "Epoch 81/110\n",
      " - 71s - loss: 0.0822 - acc: 0.9698 - val_loss: 0.0824 - val_acc: 0.9690\n",
      "Epoch 82/110\n",
      " - 71s - loss: 0.0819 - acc: 0.9699 - val_loss: 0.0817 - val_acc: 0.9693\n",
      "Epoch 83/110\n",
      " - 70s - loss: 0.0820 - acc: 0.9699 - val_loss: 0.0820 - val_acc: 0.9691\n",
      "Epoch 84/110\n",
      " - 71s - loss: 0.0820 - acc: 0.9698 - val_loss: 0.0818 - val_acc: 0.9692\n",
      "Epoch 85/110\n",
      " - 70s - loss: 0.0818 - acc: 0.9698 - val_loss: 0.0816 - val_acc: 0.9691\n",
      "Epoch 86/110\n",
      " - 70s - loss: 0.0823 - acc: 0.9698 - val_loss: 0.0818 - val_acc: 0.9692\n",
      "Epoch 87/110\n",
      " - 70s - loss: 0.0831 - acc: 0.9696 - val_loss: 0.0821 - val_acc: 0.9691\n",
      "Epoch 88/110\n",
      " - 70s - loss: 0.0818 - acc: 0.9699 - val_loss: 0.0817 - val_acc: 0.9692\n",
      "Epoch 89/110\n",
      " - 70s - loss: 0.0824 - acc: 0.9698 - val_loss: 0.0819 - val_acc: 0.9692\n",
      "Epoch 90/110\n",
      " - 70s - loss: 0.0817 - acc: 0.9699 - val_loss: 0.0814 - val_acc: 0.9693\n",
      "Epoch 91/110\n",
      " - 70s - loss: 0.0822 - acc: 0.9700 - val_loss: 0.0813 - val_acc: 0.9694\n",
      "Epoch 92/110\n",
      " - 70s - loss: 0.0809 - acc: 0.9701 - val_loss: 0.0812 - val_acc: 0.9694\n",
      "Epoch 93/110\n",
      " - 70s - loss: 0.0814 - acc: 0.9701 - val_loss: 0.0813 - val_acc: 0.9693\n",
      "Epoch 94/110\n",
      " - 70s - loss: 0.0809 - acc: 0.9701 - val_loss: 0.0812 - val_acc: 0.9694\n",
      "Epoch 95/110\n",
      " - 71s - loss: 0.0813 - acc: 0.9700 - val_loss: 0.0812 - val_acc: 0.9694\n",
      "Epoch 96/110\n",
      " - 70s - loss: 0.0813 - acc: 0.9700 - val_loss: 0.0814 - val_acc: 0.9693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/110\n",
      " - 70s - loss: 0.0809 - acc: 0.9701 - val_loss: 0.0812 - val_acc: 0.9693\n",
      "Epoch 98/110\n",
      " - 70s - loss: 0.0808 - acc: 0.9701 - val_loss: 0.0811 - val_acc: 0.9694\n",
      "Epoch 99/110\n",
      " - 70s - loss: 0.0814 - acc: 0.9701 - val_loss: 0.0813 - val_acc: 0.9693\n",
      "Epoch 100/110\n",
      " - 70s - loss: 0.0814 - acc: 0.9700 - val_loss: 0.0812 - val_acc: 0.9694\n",
      "Epoch 101/110\n",
      " - 70s - loss: 0.0812 - acc: 0.9701 - val_loss: 0.0812 - val_acc: 0.9694\n",
      "Epoch 102/110\n",
      " - 70s - loss: 0.0809 - acc: 0.9702 - val_loss: 0.0811 - val_acc: 0.9694\n",
      "Epoch 103/110\n",
      " - 70s - loss: 0.0814 - acc: 0.9701 - val_loss: 0.0812 - val_acc: 0.9694\n",
      "Epoch 104/110\n",
      " - 70s - loss: 0.0807 - acc: 0.9702 - val_loss: 0.0812 - val_acc: 0.9694\n",
      "Epoch 105/110\n",
      " - 70s - loss: 0.0809 - acc: 0.9702 - val_loss: 0.0812 - val_acc: 0.9694\n",
      "Epoch 106/110\n",
      " - 70s - loss: 0.0815 - acc: 0.9700 - val_loss: 0.0813 - val_acc: 0.9694\n",
      "Epoch 107/110\n",
      " - 70s - loss: 0.0822 - acc: 0.9699 - val_loss: 0.0813 - val_acc: 0.9694\n",
      "Epoch 108/110\n",
      " - 70s - loss: 0.0813 - acc: 0.9701 - val_loss: 0.0812 - val_acc: 0.9694\n",
      "Epoch 109/110\n",
      " - 70s - loss: 0.0807 - acc: 0.9702 - val_loss: 0.0812 - val_acc: 0.9693\n",
      "Epoch 110/110\n",
      " - 70s - loss: 0.0812 - acc: 0.9701 - val_loss: 0.0812 - val_acc: 0.9694\n",
      "Training fold 4 completed. macro f1 score : 0.94056\n",
      "Our training dataset shape is (1000, 4000, 19)\n",
      "Our validation dataset shape is (250, 4000, 19)\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2557 of 3912 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 106s - loss: 0.5933 - acc: 0.8352 - val_loss: 0.9926 - val_acc: 0.8604\n",
      "Epoch 2/110\n",
      " - 70s - loss: 0.2011 - acc: 0.9560 - val_loss: 0.5674 - val_acc: 0.9381\n",
      "Epoch 3/110\n",
      " - 70s - loss: 0.1561 - acc: 0.9630 - val_loss: 0.2655 - val_acc: 0.9573\n",
      "Epoch 4/110\n",
      " - 70s - loss: 0.1393 - acc: 0.9649 - val_loss: 0.1556 - val_acc: 0.9660\n",
      "Epoch 5/110\n",
      " - 70s - loss: 0.1303 - acc: 0.9656 - val_loss: 0.1293 - val_acc: 0.9661\n",
      "Epoch 6/110\n",
      " - 70s - loss: 0.1255 - acc: 0.9659 - val_loss: 0.1120 - val_acc: 0.9670\n",
      "Epoch 7/110\n",
      " - 70s - loss: 0.1220 - acc: 0.9661 - val_loss: 0.1043 - val_acc: 0.9673\n",
      "Epoch 8/110\n",
      " - 70s - loss: 0.1162 - acc: 0.9664 - val_loss: 0.1029 - val_acc: 0.9672\n",
      "Epoch 9/110\n",
      " - 70s - loss: 0.1142 - acc: 0.9666 - val_loss: 0.1023 - val_acc: 0.9672\n",
      "Epoch 10/110\n",
      " - 70s - loss: 0.1126 - acc: 0.9665 - val_loss: 0.1034 - val_acc: 0.9669\n",
      "Epoch 11/110\n",
      " - 70s - loss: 0.1102 - acc: 0.9666 - val_loss: 0.0972 - val_acc: 0.9676\n",
      "Epoch 12/110\n",
      " - 70s - loss: 0.1755 - acc: 0.9516 - val_loss: 0.1292 - val_acc: 0.9659\n",
      "Epoch 13/110\n",
      " - 70s - loss: 0.1234 - acc: 0.9653 - val_loss: 0.1054 - val_acc: 0.9672\n",
      "Epoch 14/110\n",
      " - 70s - loss: 0.1141 - acc: 0.9664 - val_loss: 0.0989 - val_acc: 0.9673\n",
      "Epoch 15/110\n",
      " - 70s - loss: 0.1099 - acc: 0.9666 - val_loss: 0.0982 - val_acc: 0.9673\n",
      "Epoch 16/110\n",
      " - 70s - loss: 0.1077 - acc: 0.9667 - val_loss: 0.0971 - val_acc: 0.9673\n",
      "Epoch 17/110\n",
      " - 70s - loss: 0.1067 - acc: 0.9666 - val_loss: 0.0941 - val_acc: 0.9675\n",
      "Epoch 18/110\n",
      " - 70s - loss: 0.1071 - acc: 0.9667 - val_loss: 0.0951 - val_acc: 0.9673\n",
      "Epoch 19/110\n",
      " - 70s - loss: 0.1046 - acc: 0.9668 - val_loss: 0.0937 - val_acc: 0.9675\n",
      "Epoch 20/110\n",
      " - 70s - loss: 0.1029 - acc: 0.9669 - val_loss: 0.0974 - val_acc: 0.9668\n",
      "Epoch 21/110\n",
      " - 70s - loss: 0.1026 - acc: 0.9668 - val_loss: 0.0931 - val_acc: 0.9674\n",
      "Epoch 22/110\n",
      " - 70s - loss: 0.1017 - acc: 0.9668 - val_loss: 0.0931 - val_acc: 0.9675\n",
      "Epoch 23/110\n",
      " - 70s - loss: 0.0998 - acc: 0.9671 - val_loss: 0.0933 - val_acc: 0.9674\n",
      "Epoch 24/110\n",
      " - 70s - loss: 0.0998 - acc: 0.9671 - val_loss: 0.0910 - val_acc: 0.9677\n",
      "Epoch 25/110\n",
      " - 70s - loss: 0.1001 - acc: 0.9669 - val_loss: 0.0911 - val_acc: 0.9675\n",
      "Epoch 26/110\n",
      " - 70s - loss: 0.0994 - acc: 0.9669 - val_loss: 0.0935 - val_acc: 0.9670\n",
      "Epoch 27/110\n",
      " - 70s - loss: 0.0993 - acc: 0.9669 - val_loss: 0.0947 - val_acc: 0.9666\n",
      "Epoch 28/110\n",
      " - 71s - loss: 0.0981 - acc: 0.9669 - val_loss: 0.0917 - val_acc: 0.9674\n",
      "Epoch 29/110\n",
      " - 70s - loss: 0.0971 - acc: 0.9671 - val_loss: 0.0909 - val_acc: 0.9672\n",
      "Epoch 30/110\n",
      " - 70s - loss: 0.0968 - acc: 0.9672 - val_loss: 0.0897 - val_acc: 0.9675\n",
      "Epoch 31/110\n",
      " - 70s - loss: 0.0944 - acc: 0.9674 - val_loss: 0.0879 - val_acc: 0.9679\n",
      "Epoch 32/110\n",
      " - 70s - loss: 0.0948 - acc: 0.9673 - val_loss: 0.0879 - val_acc: 0.9678\n",
      "Epoch 33/110\n",
      " - 70s - loss: 0.0945 - acc: 0.9674 - val_loss: 0.0873 - val_acc: 0.9678\n",
      "Epoch 34/110\n",
      " - 71s - loss: 0.0934 - acc: 0.9675 - val_loss: 0.0872 - val_acc: 0.9678\n",
      "Epoch 35/110\n",
      " - 70s - loss: 0.0939 - acc: 0.9674 - val_loss: 0.0877 - val_acc: 0.9679\n",
      "Epoch 36/110\n",
      " - 70s - loss: 0.0945 - acc: 0.9672 - val_loss: 0.0880 - val_acc: 0.9678\n",
      "Epoch 37/110\n",
      " - 70s - loss: 0.0933 - acc: 0.9675 - val_loss: 0.0870 - val_acc: 0.9679\n",
      "Epoch 38/110\n",
      " - 70s - loss: 0.0931 - acc: 0.9675 - val_loss: 0.0867 - val_acc: 0.9679\n",
      "Epoch 39/110\n",
      " - 70s - loss: 0.0928 - acc: 0.9675 - val_loss: 0.0867 - val_acc: 0.9679\n",
      "Epoch 40/110\n",
      " - 70s - loss: 0.0927 - acc: 0.9675 - val_loss: 0.0865 - val_acc: 0.9678\n",
      "Epoch 41/110\n",
      " - 71s - loss: 0.0918 - acc: 0.9676 - val_loss: 0.0861 - val_acc: 0.9680\n",
      "Epoch 42/110\n",
      " - 70s - loss: 0.0922 - acc: 0.9676 - val_loss: 0.0863 - val_acc: 0.9680\n",
      "Epoch 43/110\n",
      " - 70s - loss: 0.0919 - acc: 0.9675 - val_loss: 0.0863 - val_acc: 0.9679\n",
      "Epoch 44/110\n",
      " - 70s - loss: 0.0927 - acc: 0.9674 - val_loss: 0.0868 - val_acc: 0.9678\n",
      "Epoch 45/110\n",
      " - 70s - loss: 0.0921 - acc: 0.9675 - val_loss: 0.0897 - val_acc: 0.9670\n",
      "Epoch 46/110\n",
      " - 70s - loss: 0.0918 - acc: 0.9675 - val_loss: 0.0897 - val_acc: 0.9671\n",
      "Epoch 47/110\n",
      " - 70s - loss: 0.0918 - acc: 0.9676 - val_loss: 0.0861 - val_acc: 0.9679\n",
      "Epoch 48/110\n",
      " - 70s - loss: 0.0917 - acc: 0.9676 - val_loss: 0.0871 - val_acc: 0.9677\n",
      "Epoch 49/110\n",
      " - 70s - loss: 0.0911 - acc: 0.9677 - val_loss: 0.0863 - val_acc: 0.9678\n",
      "Epoch 50/110\n",
      " - 70s - loss: 0.0912 - acc: 0.9676 - val_loss: 0.0860 - val_acc: 0.9679\n",
      "Epoch 51/110\n",
      " - 70s - loss: 0.0909 - acc: 0.9677 - val_loss: 0.0857 - val_acc: 0.9679\n",
      "Epoch 52/110\n",
      " - 70s - loss: 0.0909 - acc: 0.9677 - val_loss: 0.0856 - val_acc: 0.9680\n",
      "Epoch 53/110\n",
      " - 70s - loss: 0.0911 - acc: 0.9676 - val_loss: 0.0857 - val_acc: 0.9679\n",
      "Epoch 54/110\n",
      " - 70s - loss: 0.0901 - acc: 0.9677 - val_loss: 0.0858 - val_acc: 0.9679\n",
      "Epoch 55/110\n",
      " - 70s - loss: 0.0910 - acc: 0.9676 - val_loss: 0.0860 - val_acc: 0.9679\n",
      "Epoch 56/110\n",
      " - 70s - loss: 0.0907 - acc: 0.9677 - val_loss: 0.0854 - val_acc: 0.9680\n",
      "Epoch 57/110\n",
      " - 70s - loss: 0.0908 - acc: 0.9677 - val_loss: 0.0861 - val_acc: 0.9678\n",
      "Epoch 58/110\n",
      " - 70s - loss: 0.0911 - acc: 0.9676 - val_loss: 0.0854 - val_acc: 0.9680\n",
      "Epoch 59/110\n",
      " - 71s - loss: 0.0899 - acc: 0.9678 - val_loss: 0.0852 - val_acc: 0.9680\n",
      "Epoch 60/110\n",
      " - 70s - loss: 0.0905 - acc: 0.9677 - val_loss: 0.0868 - val_acc: 0.9676\n",
      "Epoch 61/110\n",
      " - 70s - loss: 0.0900 - acc: 0.9677 - val_loss: 0.0852 - val_acc: 0.9680\n",
      "Epoch 62/110\n",
      " - 70s - loss: 0.0897 - acc: 0.9678 - val_loss: 0.0851 - val_acc: 0.9680\n",
      "Epoch 63/110\n",
      " - 70s - loss: 0.0899 - acc: 0.9678 - val_loss: 0.0853 - val_acc: 0.9679\n",
      "Epoch 64/110\n",
      " - 70s - loss: 0.0894 - acc: 0.9678 - val_loss: 0.0850 - val_acc: 0.9680\n",
      "Epoch 65/110\n",
      " - 70s - loss: 0.0895 - acc: 0.9678 - val_loss: 0.0854 - val_acc: 0.9679\n",
      "Epoch 66/110\n",
      " - 70s - loss: 0.0897 - acc: 0.9678 - val_loss: 0.0853 - val_acc: 0.9679\n",
      "Epoch 67/110\n",
      " - 70s - loss: 0.0893 - acc: 0.9678 - val_loss: 0.0850 - val_acc: 0.9680\n",
      "Epoch 68/110\n",
      " - 70s - loss: 0.0894 - acc: 0.9678 - val_loss: 0.0851 - val_acc: 0.9680\n",
      "Epoch 69/110\n",
      " - 70s - loss: 0.0894 - acc: 0.9679 - val_loss: 0.0853 - val_acc: 0.9680\n",
      "Epoch 70/110\n",
      " - 70s - loss: 0.0888 - acc: 0.9678 - val_loss: 0.0854 - val_acc: 0.9679\n",
      "Epoch 71/110\n",
      " - 70s - loss: 0.0889 - acc: 0.9678 - val_loss: 0.0851 - val_acc: 0.9680\n",
      "Epoch 72/110\n",
      " - 70s - loss: 0.0885 - acc: 0.9680 - val_loss: 0.0847 - val_acc: 0.9680\n",
      "Epoch 73/110\n",
      " - 70s - loss: 0.0886 - acc: 0.9679 - val_loss: 0.0850 - val_acc: 0.9680\n",
      "Epoch 74/110\n",
      " - 70s - loss: 0.0889 - acc: 0.9678 - val_loss: 0.0848 - val_acc: 0.9680\n",
      "Epoch 75/110\n",
      " - 74s - loss: 0.0891 - acc: 0.9679 - val_loss: 0.0847 - val_acc: 0.9681\n",
      "Epoch 76/110\n",
      " - 70s - loss: 0.0881 - acc: 0.9680 - val_loss: 0.0847 - val_acc: 0.9681\n",
      "Epoch 77/110\n",
      " - 71s - loss: 0.0888 - acc: 0.9679 - val_loss: 0.0846 - val_acc: 0.9681\n",
      "Epoch 78/110\n",
      " - 70s - loss: 0.0884 - acc: 0.9679 - val_loss: 0.0845 - val_acc: 0.9681\n",
      "Epoch 79/110\n",
      " - 70s - loss: 0.0888 - acc: 0.9679 - val_loss: 0.0852 - val_acc: 0.9679\n",
      "Epoch 80/110\n",
      " - 70s - loss: 0.0882 - acc: 0.9680 - val_loss: 0.0846 - val_acc: 0.9680\n",
      "Epoch 81/110\n",
      " - 70s - loss: 0.0893 - acc: 0.9677 - val_loss: 0.0845 - val_acc: 0.9681\n",
      "Epoch 82/110\n",
      " - 70s - loss: 0.0881 - acc: 0.9680 - val_loss: 0.0845 - val_acc: 0.9681\n",
      "Epoch 83/110\n",
      " - 70s - loss: 0.0882 - acc: 0.9680 - val_loss: 0.0844 - val_acc: 0.9681\n",
      "Epoch 84/110\n",
      " - 70s - loss: 0.0882 - acc: 0.9680 - val_loss: 0.0845 - val_acc: 0.9680\n",
      "Epoch 85/110\n",
      " - 70s - loss: 0.0878 - acc: 0.9681 - val_loss: 0.0843 - val_acc: 0.9681\n",
      "Epoch 86/110\n",
      " - 70s - loss: 0.0877 - acc: 0.9680 - val_loss: 0.0846 - val_acc: 0.9681\n",
      "Epoch 87/110\n",
      " - 70s - loss: 0.0882 - acc: 0.9680 - val_loss: 0.0845 - val_acc: 0.9681\n",
      "Epoch 88/110\n",
      " - 71s - loss: 0.0882 - acc: 0.9680 - val_loss: 0.0846 - val_acc: 0.9680\n",
      "Epoch 89/110\n",
      " - 70s - loss: 0.0883 - acc: 0.9679 - val_loss: 0.0842 - val_acc: 0.9681\n",
      "Epoch 90/110\n",
      " - 70s - loss: 0.0883 - acc: 0.9679 - val_loss: 0.0848 - val_acc: 0.9680\n",
      "Epoch 91/110\n",
      " - 70s - loss: 0.0878 - acc: 0.9680 - val_loss: 0.0841 - val_acc: 0.9681\n",
      "Epoch 92/110\n",
      " - 70s - loss: 0.0869 - acc: 0.9682 - val_loss: 0.0841 - val_acc: 0.9682\n",
      "Epoch 93/110\n",
      " - 70s - loss: 0.0870 - acc: 0.9682 - val_loss: 0.0841 - val_acc: 0.9682\n",
      "Epoch 94/110\n",
      " - 70s - loss: 0.0873 - acc: 0.9681 - val_loss: 0.0841 - val_acc: 0.9681\n",
      "Epoch 95/110\n",
      " - 71s - loss: 0.0870 - acc: 0.9681 - val_loss: 0.0841 - val_acc: 0.9681\n",
      "Epoch 96/110\n",
      " - 70s - loss: 0.0872 - acc: 0.9681 - val_loss: 0.0840 - val_acc: 0.9681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/110\n",
      " - 70s - loss: 0.0877 - acc: 0.9681 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Epoch 98/110\n",
      " - 70s - loss: 0.0874 - acc: 0.9681 - val_loss: 0.0841 - val_acc: 0.9681\n",
      "Epoch 99/110\n",
      " - 70s - loss: 0.0874 - acc: 0.9681 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Epoch 100/110\n",
      " - 71s - loss: 0.0871 - acc: 0.9682 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Epoch 101/110\n",
      " - 70s - loss: 0.0876 - acc: 0.9681 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Epoch 102/110\n",
      " - 70s - loss: 0.0875 - acc: 0.9681 - val_loss: 0.0840 - val_acc: 0.9682\n",
      "Epoch 103/110\n",
      " - 70s - loss: 0.0875 - acc: 0.9680 - val_loss: 0.0840 - val_acc: 0.9682\n",
      "Epoch 104/110\n",
      " - 70s - loss: 0.0882 - acc: 0.9680 - val_loss: 0.0840 - val_acc: 0.9682\n",
      "Epoch 105/110\n",
      " - 70s - loss: 0.0873 - acc: 0.9681 - val_loss: 0.0840 - val_acc: 0.9682\n",
      "Epoch 106/110\n",
      " - 70s - loss: 0.0875 - acc: 0.9681 - val_loss: 0.0841 - val_acc: 0.9682\n",
      "Epoch 107/110\n",
      " - 70s - loss: 0.0870 - acc: 0.9682 - val_loss: 0.0840 - val_acc: 0.9682\n",
      "Epoch 108/110\n",
      " - 70s - loss: 0.0877 - acc: 0.9682 - val_loss: 0.0841 - val_acc: 0.9681\n",
      "Epoch 109/110\n",
      " - 71s - loss: 0.0871 - acc: 0.9681 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Epoch 110/110\n",
      " - 70s - loss: 0.0872 - acc: 0.9682 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Training fold 5 completed. macro f1 score : 0.93834\n",
      "Training completed. oof macro f1 score : 0.93944\n",
      "Training completed...\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "def read_data():\n",
    "    train = pd.read_csv('./data/train_clean.csv',\n",
    "                        dtype={\n",
    "                            'time': np.float32,\n",
    "                            'signal': np.float32,\n",
    "                            'open_channels': np.int32\n",
    "                        })\n",
    "    test = pd.read_csv('./data/test_clean.csv',\n",
    "                       dtype={\n",
    "                           'time': np.float32,\n",
    "                           'signal': np.float32\n",
    "                       })\n",
    "    sub = pd.read_csv('./data/sample_submission.csv',\n",
    "                      dtype={'time': np.float32})\n",
    "\n",
    "    Y_train_proba = np.load(\"./data/Y_train_proba.npy\")\n",
    "    Y_test_proba = np.load(\"./data/Y_test_proba.npy\")\n",
    "\n",
    "    for i in range(11):\n",
    "        train[f\"proba_{i}\"] = Y_train_proba[:, i]\n",
    "        test[f\"proba_{i}\"] = Y_test_proba[:, i]\n",
    "\n",
    "    return train, test, sub\n",
    "\n",
    "\n",
    "# create batches of 4000 observations\n",
    "def batching(df, batch_size):\n",
    "    df['group'] = df.groupby(df.index // batch_size,\n",
    "                             sort=False)['signal'].agg(['ngroup']).values\n",
    "    df['group'] = df['group'].astype(np.uint16)\n",
    "    return df\n",
    "\n",
    "\n",
    "# normalize the data (standard scaler). We can also try other scalers for a better score!\n",
    "def normalize(train, test):\n",
    "    train_input_mean = train.signal.mean()\n",
    "    train_input_sigma = train.signal.std()\n",
    "    train['signal'] = (train.signal - train_input_mean) / train_input_sigma\n",
    "    test['signal'] = (test.signal - train_input_mean) / train_input_sigma\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# get lead and lags features\n",
    "def lag_with_pct_change(df, windows):\n",
    "    for window in windows:\n",
    "        df['signal_shift_pos_' +\n",
    "           str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n",
    "        df['signal_shift_neg_' +\n",
    "           str(window)] = df.groupby('group')['signal'].shift(-1 *\n",
    "                                                              window).fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\n",
    "def run_feat_engineering(df, batch_size):\n",
    "    # create batches\n",
    "    df = batching(df, batch_size=batch_size)\n",
    "    # create leads and lags (1, 2, 3 making them 6 features)\n",
    "    df = lag_with_pct_change(df, [1, 2, 3])\n",
    "    # create signal ** 2 (this is the new feature)\n",
    "    df['signal_2'] = df['signal']**2\n",
    "    return df\n",
    "\n",
    "\n",
    "# fillna with the mean and select features for training\n",
    "def feature_selection(train, test):\n",
    "    features = [\n",
    "        col for col in train.columns\n",
    "        if col not in ['index', 'group', 'open_channels', 'time']\n",
    "    ]\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "    test = test.replace([np.inf, -np.inf], np.nan)\n",
    "    for feature in features:\n",
    "        feature_mean = pd.concat([train[feature], test[feature]],\n",
    "                                 axis=0).mean()\n",
    "        train[feature] = train[feature].fillna(feature_mean)\n",
    "        test[feature] = test[feature].fillna(feature_mean)\n",
    "    return train, test, features\n",
    "\n",
    "\n",
    "# model function (very important, you can try different arquitectures to get a better score. I believe that top public leaderboard is a 1D Conv + RNN style)\n",
    "def Classifier(shape_):\n",
    "    def cbr(x, out_layer, kernel, stride, dilation):\n",
    "        x = Conv1D(out_layer,\n",
    "                   kernel_size=kernel,\n",
    "                   dilation_rate=dilation,\n",
    "                   strides=stride,\n",
    "                   padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def wave_block(x, filters, kernel_size, n):\n",
    "        dilation_rates = [2**i for i in range(n)]\n",
    "        x = Conv1D(filters=filters, kernel_size=1, padding='same')(x)\n",
    "        res_x = x\n",
    "        for dilation_rate in dilation_rates:\n",
    "            tanh_out = Conv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              padding='same',\n",
    "                              activation='tanh',\n",
    "                              dilation_rate=dilation_rate)(x)\n",
    "            sigm_out = Conv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              padding='same',\n",
    "                              activation='sigmoid',\n",
    "                              dilation_rate=dilation_rate)(x)\n",
    "            x = Multiply()([tanh_out, sigm_out])\n",
    "            x = Conv1D(filters=filters, kernel_size=1, padding='same')(x)\n",
    "            res_x = Add()([res_x, x])\n",
    "        return res_x\n",
    "\n",
    "    inp = Input(shape=(shape_))\n",
    "    x = cbr(inp, 64, 7, 1, 1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 16, 3, 12)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 32, 3, 8)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 64, 3, 4)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 128, 3, 1)\n",
    "    x = cbr(x, 32, 7, 1, 1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(11, activation='softmax', name='out')(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    opt = Adam(lr=LR)\n",
    "#     opt = tfa.optimizers.SWA(opt)\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# function that decrease the learning as epochs increase (i also change this part of the code)\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 30:\n",
    "        lr = LR\n",
    "    elif epoch < 40:\n",
    "        lr = LR / 3\n",
    "    elif epoch < 50:\n",
    "        lr = LR / 5\n",
    "    elif epoch < 60:\n",
    "        lr = LR / 7\n",
    "    elif epoch < 70:\n",
    "        lr = LR / 9\n",
    "    elif epoch < 80:\n",
    "        lr = LR / 11\n",
    "    elif epoch < 90:\n",
    "        lr = LR / 13\n",
    "    else:\n",
    "        lr = LR / 100\n",
    "    return lr\n",
    "\n",
    "\n",
    "# class to get macro f1 score. This is not entirely necessary but it's fun to check f1 score of each epoch (be carefull, if you use this function early stopping callback will not work)\n",
    "class MacroF1(Callback):\n",
    "    def __init__(self, model, inputs, targets):\n",
    "        self.model = model\n",
    "        self.inputs = inputs\n",
    "        self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
    "        score = f1_score(self.targets, pred, average='macro')\n",
    "        print(f'F1 Macro Score: {score:.5f}')\n",
    "\n",
    "\n",
    "# main function to perfrom groupkfold cross validation (we have 1000 vectores of 4000 rows and 8 features (columns)). Going to make 5 groups with this subgroups.\n",
    "def run_cv_model_by_batch(train, test, splits, batch_col, feats,\n",
    "                          sample_submission, nn_epochs, nn_batch_size):\n",
    "\n",
    "    seed_everything(SEED)\n",
    "    K.clear_session()\n",
    "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                                      inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(),\n",
    "                                config=config)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "    oof_ = np.zeros(\n",
    "        (len(train), 11)\n",
    "    )  # build out of folds matrix with 11 columns, they represent our target variables classes (from 0 to 10)\n",
    "    preds_ = np.zeros((len(test), 11))\n",
    "    target = ['open_channels']\n",
    "    group = train['group']\n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    splits = [x for x in kf.split(train, train[target], group)]\n",
    "\n",
    "    new_splits = []\n",
    "    for sp in splits:\n",
    "        new_split = []\n",
    "        new_split.append(np.unique(group[sp[0]]))\n",
    "        new_split.append(np.unique(group[sp[1]]))\n",
    "        new_split.append(sp[1])\n",
    "        new_splits.append(new_split)\n",
    "    # pivot target columns to transform the net to a multiclass classification estructure (you can also leave it in 1 vector with sparsecategoricalcrossentropy loss function)\n",
    "    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]],\n",
    "                   axis=1)\n",
    "\n",
    "    tr.columns = ['target_' + str(i) for i in range(11)] + ['group']\n",
    "    target_cols = ['target_' + str(i) for i in range(11)]\n",
    "    train_tr = np.array(\n",
    "        list(tr.groupby('group').apply(\n",
    "            lambda x: x[target_cols].values))).astype(np.float32)\n",
    "    train = np.array(\n",
    "        list(train.groupby('group').apply(lambda x: x[feats].values)))\n",
    "    test = np.array(\n",
    "        list(test.groupby('group').apply(lambda x: x[feats].values)))\n",
    "\n",
    "    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:],\n",
    "                                                             start=0):\n",
    "        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n",
    "        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n",
    "        print(f'Our training dataset shape is {train_x.shape}')\n",
    "        print(f'Our validation dataset shape is {valid_x.shape}')\n",
    "\n",
    "        gc.collect()\n",
    "        shape_ = (\n",
    "            None, train_x.shape[2]\n",
    "        )  # input is going to be the number of feature we are using (dimension 2 of 0, 1, 2)\n",
    "        model = Classifier(shape_)\n",
    "        # using our lr_schedule function\n",
    "        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
    "        model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            epochs=nn_epochs,\n",
    "            callbacks=[\n",
    "                cb_lr_schedule,\n",
    "                EarlyStopping(monitor='val_accuracy', mode='max', verbose=1)\n",
    "            ],  # adding custom evaluation metric for each epoch\n",
    "            batch_size=nn_batch_size,\n",
    "            verbose=2,\n",
    "            validation_data=(valid_x, valid_y))\n",
    "        \n",
    "        preds_f = model.predict(valid_x)\n",
    "        f1_score_ = f1_score(\n",
    "            np.argmax(valid_y, axis=2).reshape(-1),\n",
    "            np.argmax(preds_f, axis=2).reshape(-1),\n",
    "            average='macro'\n",
    "        )  # need to get the class with the biggest probability\n",
    "        print(\n",
    "            f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}'\n",
    "        )\n",
    "        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n",
    "        oof_[val_orig_idx, :] += preds_f\n",
    "        te_preds = model.predict(test)\n",
    "        te_preds = te_preds.reshape(-1, te_preds.shape[-1])\n",
    "        preds_ += te_preds / SPLITS\n",
    "    # calculate the oof macro f1_score\n",
    "    f1_score_ = f1_score(\n",
    "        np.argmax(train_tr, axis=2).reshape(-1),\n",
    "        np.argmax(oof_, axis=1),\n",
    "        average='macro'\n",
    "    )  # axis 2 for the 3 Dimension array and axis 1 for the 2 Domension Array (extracting the best class)\n",
    "    print(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n",
    "    sample_submission['open_channels'] = np.argmax(preds_, axis=1).astype(int)\n",
    "    sample_submission.to_csv('submission_wavenet.csv',\n",
    "                             index=False,\n",
    "                             float_format='%.4f')\n",
    "\n",
    "\n",
    "# this function run our entire program\n",
    "def run_everything():\n",
    "\n",
    "    print('Reading Data Started...')\n",
    "    train, test, sample_submission = read_data()\n",
    "    train, test = normalize(train, test)\n",
    "    print('Reading and Normalizing Data Completed')\n",
    "\n",
    "    print('Creating Features')\n",
    "    print('Feature Engineering Started...')\n",
    "    train = run_feat_engineering(train, batch_size=GROUP_BATCH_SIZE)\n",
    "    test = run_feat_engineering(test, batch_size=GROUP_BATCH_SIZE)\n",
    "    train, test, features = feature_selection(train, test)\n",
    "    print('Feature Engineering Completed...')\n",
    "\n",
    "    print(\n",
    "        f'Training Wavenet model with {SPLITS} folds of GroupKFold Started...')\n",
    "    run_cv_model_by_batch(train, test, SPLITS, 'group', features,\n",
    "                          sample_submission, EPOCHS, NNBATCHSIZE)\n",
    "    print('Training completed...')\n",
    "\n",
    "\n",
    "run_everything()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-ion",
   "language": "python",
   "name": "conda-env-ion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
