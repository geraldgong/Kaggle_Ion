{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T09:56:33.663962Z",
     "start_time": "2020-05-11T09:56:33.661056Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    \n",
    "import gc\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T09:56:35.899696Z",
     "start_time": "2020-05-11T09:56:35.894828Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "nvidia_gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if nvidia_gpu:\n",
    "    from tensorflow.keras.layers import *\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import random\n",
    "    from tensorflow.keras.callbacks import Callback, LearningRateScheduler, EarlyStopping\n",
    "    from tensorflow.keras.losses import categorical_crossentropy\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras import backend as K\n",
    "    from tensorflow.keras import losses, models, optimizers\n",
    "\n",
    "else:\n",
    "    import plaidml.keras\n",
    "    plaidml.keras.install_backend()\n",
    "    import os\n",
    "    os.environ['KERAS_BACKEND'] = 'plaidml.keras.backend'\n",
    "\n",
    "    import keras\n",
    "    from keras.layers import *\n",
    "    from keras.callbacks import Callback, LearningRateScheduler, EarlyStopping\n",
    "    from keras.losses import categorical_crossentropy\n",
    "    from keras.optimizers import Adam\n",
    "    from keras import backend as K\n",
    "    from keras import losses, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T09:59:50.251789Z",
     "start_time": "2020-05-11T09:59:50.227177Z"
    }
   },
   "outputs": [],
   "source": [
    "# configurations and main hyperparammeters\n",
    "EPOCHS = 180\n",
    "NNBATCHSIZE = 16\n",
    "GROUP_BATCH_SIZE = 4000\n",
    "SEED = 321\n",
    "LR = 0.0015\n",
    "SPLITS = 6\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "def Classifier(shape_):\n",
    "    def cbr(x, out_layer, kernel, stride, dilation):\n",
    "        # Case-based reasoning\n",
    "        x = Conv1D(out_layer,\n",
    "                   kernel_size=kernel,\n",
    "                   dilation_rate=dilation,\n",
    "                   strides=stride,\n",
    "                   padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def wave_block(x, filters, kernel_size, n):\n",
    "        dilation_rates = [2**i for i in range(n)]\n",
    "        x = Conv1D(filters=filters, kernel_size=1, padding='same')(x)\n",
    "        res_x = x\n",
    "        for dilation_rate in dilation_rates:\n",
    "            tanh_out = Conv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              padding='same',\n",
    "                              activation='tanh',\n",
    "                              dilation_rate=dilation_rate)(x)\n",
    "            sigm_out = Conv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              padding='same',\n",
    "                              activation='sigmoid',\n",
    "                              dilation_rate=dilation_rate)(x)\n",
    "            x = Multiply()([tanh_out, sigm_out])\n",
    "            x = Conv1D(filters=filters, kernel_size=1, padding='same')(x)\n",
    "            res_x = Add()([res_x, x])\n",
    "        return res_x\n",
    "\n",
    "    inp = Input(shape=(shape_))\n",
    "    x = cbr(inp, 64, 7, 1, 1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 16, 3, 12)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 32, 3, 8)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 64, 3, 4)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 128, 3, 1)\n",
    "    x = cbr(x, 32, 7, 1, 1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(11, activation='softmax', name='out')(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    opt = Adam(lr=LR)\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# function that decrease the learning as epochs increase\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 30:\n",
    "        lr = LR\n",
    "    elif epoch < 40:\n",
    "        lr = LR / 3\n",
    "    elif epoch < 50:\n",
    "        lr = LR / 5\n",
    "    elif epoch < 60:\n",
    "        lr = LR / 7\n",
    "    elif epoch < 70:\n",
    "        lr = LR / 9\n",
    "    elif epoch < 80:\n",
    "        lr = LR / 11\n",
    "    elif epoch < 90:\n",
    "        lr = LR / 13\n",
    "    else:\n",
    "        lr = LR / 100\n",
    "    return lr\n",
    "\n",
    "\n",
    "# class to get macro f1 score. This is not entirely necessary but\n",
    "# it's fun to check f1 score of each epoch (be carefull,\n",
    "# if you use this function early stopping callback will not work)\n",
    "class MacroF1(Callback):\n",
    "    def __init__(self, model, inputs, targets):\n",
    "        self.model = model\n",
    "        self.inputs = inputs\n",
    "        self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
    "        score = f1_score(self.targets, pred, average='macro')\n",
    "        print(f'F1 Macro Score: {score:.5f}')\n",
    "\n",
    "\n",
    "# main function to perfrom groupkfold cross validation\n",
    "# (we have 1000 vectores of 4000 rows and 8 features (columns)).\n",
    "# Going to make 5 groups with this subgroups.\n",
    "def run_cv_model_by_batch(train, test, splits, batch_col, feats,\n",
    "                          sample_submission, nn_epochs, nn_batch_size):\n",
    "\n",
    "    seed_everything(SEED)\n",
    "    K.clear_session()\n",
    "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                                      inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(),\n",
    "                                config=config)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "    oof_ = np.zeros((len(train), 11))\n",
    "    # build out of folds matrix with 11 columns,\n",
    "    #they represent our target variables classes (from 0 to 10)\n",
    "    preds_ = np.zeros((len(test), 11))\n",
    "    target = ['open_channels']\n",
    "    group = train['group']\n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    splits = [x for x in kf.split(train, train[target], group)]\n",
    "\n",
    "    new_splits = []\n",
    "    for sp in splits:\n",
    "        new_split = []\n",
    "        new_split.append(np.unique(group[sp[0]]))\n",
    "        new_split.append(np.unique(group[sp[1]]))\n",
    "        new_split.append(sp[1])\n",
    "        new_splits.append(new_split)\n",
    "\n",
    "    # pivot target columns to transform the net to a multiclass classification\n",
    "    # estructure (you can also leave it in 1 vector with\n",
    "    # sparsecategoricalcrossentropy loss function)\n",
    "    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]],\n",
    "                   axis=1)\n",
    "\n",
    "    tr.columns = ['target_' + str(i) for i in range(11)] + ['group']\n",
    "    target_cols = ['target_' + str(i) for i in range(11)]\n",
    "    train_tr = np.array(\n",
    "        list(tr.groupby('group').apply(\n",
    "            lambda x: x[target_cols].values))).astype(np.float32)\n",
    "    train = np.array(\n",
    "        list(train.groupby('group').apply(lambda x: x[feats].values)))\n",
    "    test = np.array(\n",
    "        list(test.groupby('group').apply(lambda x: x[feats].values)))\n",
    "\n",
    "    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:],\n",
    "                                                             start=0):\n",
    "        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n",
    "        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n",
    "        print(f'Our training dataset shape is {train_x.shape}')\n",
    "        print(f'Our validation dataset shape is {valid_x.shape}')\n",
    "\n",
    "        gc.collect()\n",
    "        shape_ = (None, train_x.shape[2])\n",
    "        # input is going to be the number of feature we are using\n",
    "        # (dimension 2 of 0, 1, 2)\n",
    "        model = Classifier(shape_)\n",
    "        # using our lr_schedule function\n",
    "        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
    "        model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            epochs=nn_epochs,\n",
    "            callbacks=[\n",
    "                cb_lr_schedule,\n",
    "                EarlyStopping(monitor='val_accuracy', mode='max', verbose=1)\n",
    "            ],  # adding custom evaluation metric for each epoch\n",
    "            batch_size=nn_batch_size,\n",
    "            verbose=2,\n",
    "            validation_data=(valid_x, valid_y))\n",
    "\n",
    "        preds_f = model.predict(valid_x)\n",
    "        f1_score_ = f1_score(\n",
    "            np.argmax(valid_y, axis=2).reshape(-1),\n",
    "            np.argmax(preds_f, axis=2).reshape(-1),\n",
    "            average='macro'\n",
    "        )  # need to get the class with the biggest probability\n",
    "        print(\n",
    "            f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}'\n",
    "        )\n",
    "        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n",
    "        oof_[val_orig_idx, :] += preds_f\n",
    "        te_preds = model.predict(test)\n",
    "        te_preds = te_preds.reshape(-1, te_preds.shape[-1])\n",
    "        preds_ += te_preds / SPLITS\n",
    "    # calculate the oof macro f1_score\n",
    "    f1_score_ = f1_score(np.argmax(train_tr, axis=2).reshape(-1),\n",
    "                         np.argmax(oof_, axis=1),\n",
    "                         average='macro')\n",
    "    # axis 2 for the 3 Dimension array and axis 1 for the 2 Domension Array\n",
    "    # (extracting the best class)\n",
    "    print(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n",
    "    sample_submission['open_channels'] = np.argmax(preds_, axis=1).astype(int)\n",
    "    sample_submission.to_csv('submission_wavenet.csv',\n",
    "                             index=False,\n",
    "                             float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started...')\n",
    "run_cv_model_by_batch(train, test, SPLITS, 'group', features,\n",
    "                      sample_submission, EPOCHS, NNBATCHSIZE)\n",
    "print('Training completed...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-ion",
   "language": "python",
   "name": "conda-env-ion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
